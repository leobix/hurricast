{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import data_processing as proc\n",
    "path=\"./data/last3years.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after fillna, nan entries of _WIND is 648\n",
      "after fillna, nan entries of _PRES is 537\n",
      "after interpolate, nan entries of LAT is 0\n",
      "after interpolate, nan entries of LON is 0\n",
      "after interpolate, nan entries of WMO_WIND is 283\n",
      "after interpolate, nan entries of WMO_PRES is 239\n",
      "after interpolate, nan entries of DIST2LAND is 0\n",
      "after interpolate, nan entries of STORM_SPEED is 0\n",
      "selecting storms according to min_wind, min_step, max_step requirements\n",
      "nan entries of LAT is 0\n",
      "nan entries of LON is 0\n",
      "nan entries of WMO_WIND is 241\n",
      "nan entries of WMO_PRES is 129\n",
      "nan entries of DIST2LAND is 0\n",
      "nan entries of STORM_SPEED is 0\n",
      "one-hot added for  BASIN\n",
      "The dictionary of storms has been created.\n",
      "The trajectories have now been padded.\n",
      "There are 375 storms with 26 features, and maximum number of steps is 120 and minimum is 120.\n",
      "creating tensor, dropping SID and ISO_TIME features\n",
      "The tensor has now been created.\n",
      "columns in tensor: ['SID', 'NAME', 'ISO_TIME', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'basin_AN', 'basin_EP', 'basin_NI', 'basin_SI', 'basin_SP', 'basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n"
     ]
    }
   ],
   "source": [
    "#prepare_tabular_data_vision example \n",
    "e,d = proc.prepare_tabular_data_vision(path=\"../data/last3years.csv\", min_wind=34, min_steps=20,\n",
    "                  max_steps=120, get_displacement=True, forecast=False, predict_period = 24, one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here set up the path of your file, a priori it should be on same directory\n",
    "data = pd.read_csv(\"../data/last3years.csv\")\n",
    "data.drop(0, axis=0, inplace=True)\n",
    "# Preview the first 5 lines of the loaded data \n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after fillna, nan entries of _WIND is 648\n",
      "after fillna, nan entries of _PRES is 537\n"
     ]
    }
   ],
   "source": [
    "#convert columns to numeric values\n",
    "#and interpolate missing values\n",
    "def numeric_data(data):\n",
    "    for i in ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED','STORM_DIR']:\n",
    "        data[i]=pd.to_numeric(data[i],errors='coerce').astype('float64')\n",
    "    return data\n",
    "\n",
    "#fillna of _WIND and _PRES:\n",
    "def fillna_wind_pres(df):\n",
    "    #substitute source of information\n",
    "    sub_list = ['USA','TOKYO','CMA','HKO','NEWDELHI','MLC','TD9635','NEUMANN','DS824','TD9636','WELLINGTON','NADI']\n",
    "    #for two features \n",
    "    for feature in ['_WIND','_PRES']:\n",
    "        #loop over sub sources in order \n",
    "        for sub in sub_list:\n",
    "            col_sub = pd.to_numeric(df[sub+feature],errors='coerce').astype('float64')\n",
    "            df['WMO'+feature].fillna(col_sub, inplace=True)\n",
    "        print('after fillna, nan entries of %s is %s'%(feature, df.loc[df['WMO'+feature].isnull()].shape[0]))\n",
    "    return df \n",
    "\n",
    "def interpolate_data(data):\n",
    "    for i in ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED']:\n",
    "        #use the linear interpolation value to fillna, fill 2 value on both direction \n",
    "        data[i]=data[i].interpolate(method='linear', limit=2, limit_direction='both')\n",
    "        print('after interpolate, nan entries of %s is %s'%(i, data.loc[data[i].isnull()].shape[0]))\n",
    "    return data \n",
    "\n",
    "\n",
    "# df0 is  cleaned data \n",
    "data = numeric_data(data)\n",
    "data = fillna_wind_pres(data)\n",
    "# data = interpolate_data(data)\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting storms according to min_wind, min_step, max_step requirements\n",
      "number of storms selected is 240\n",
      "nan entries of LAT is 0\n",
      "nan entries of LON is 0\n",
      "nan entries of WMO_WIND is 20\n",
      "nan entries of WMO_PRES is 20\n",
      "nan entries of DIST2LAND is 0\n",
      "nan entries of STORM_SPEED is 0\n"
     ]
    }
   ],
   "source": [
    "def interpolate(data):\n",
    "    for i in ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED']:\n",
    "        #use the linear interpolation value to fillna, fill 2 value on both direction \n",
    "        data[i]=data[i].interpolate(method='linear', limit=2, limit_direction='forward')\n",
    "    return data \n",
    "\n",
    "def check_nan(data):\n",
    "    for i in ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED']:\n",
    "            print('nan entries of %s is %s'%(i, data.loc[data[i].isnull()].shape[0]))\n",
    "    return \n",
    "\n",
    "# function to select storms based on specified requirements   \n",
    "def select_storms(data, min_wind=34, min_steps = 20, max_steps = 120):\n",
    "    print('selecting storms according to min_wind, min_step, max_step requirements')\n",
    "    #get unique storm_id:\n",
    "    SID=pd.unique(data['SID']).tolist()\n",
    "    #create empty dictionary\n",
    "    dict0={}\n",
    "    ind = 0\n",
    "    for i in range(len(SID)):\n",
    "        #get data of a particular SID\n",
    "        M = data.loc[data['SID'] == SID[i]]\n",
    "        M.reset_index(inplace=True, drop=True)\n",
    "        #use the linear interpolation value to fillna \n",
    "        M= interpolate(M) \n",
    "        length = M.loc[M['WMO_WIND'] >= min_wind].shape[0] \n",
    "        #if there is any index than min_wind \n",
    "        if length > min_steps: \n",
    "            #first index \n",
    "            i0 = M.index[M['WMO_WIND']>= min_wind][0] \n",
    "            i_max = min(max_steps, length)\n",
    "            M_selected = M.iloc[i0:i_max]\n",
    "            #record selected data \n",
    "            dict0.update({ind: M_selected})\n",
    "            ind +=1 \n",
    "            \n",
    "    print('number of storms selected is', ind) \n",
    "    \n",
    "    #concatenate dict to df \n",
    "    data_out = pd.DataFrame()\n",
    "    for i in dict0:\n",
    "        data_out = pd.concat([data_out, dict0[i]], axis=0)\n",
    "    #reset index \n",
    "    data_out.reset_index(inplace=True, drop=True)\n",
    "    #check nan entries \n",
    "    check_nan(data_out)\n",
    "    #fillna with zero \n",
    "    data_out.fillna(value=0, inplace=True)\n",
    "    return data_out \n",
    "\n",
    "data_selected= select_storms(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        37.0\n",
       "1        45.0\n",
       "2        45.0\n",
       "3        45.0\n",
       "4        45.0\n",
       "        ...  \n",
       "9830     85.0\n",
       "9831     97.0\n",
       "9832    110.0\n",
       "9833    112.0\n",
       "9834    115.0\n",
       "Name: WMO_WIND, Length: 9835, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_selected.loc[data_selected['WMO_WIND'].isnull()]\n",
    "data_selected['WMO_WIND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan entries of LAT is 0\n",
      "nan entries of LON is 0\n",
      "nan entries of WMO_WIND is 486\n",
      "nan entries of WMO_PRES is 3048\n",
      "nan entries of DIST2LAND is 0\n",
      "nan entries of STORM_SPEED is 0\n"
     ]
    }
   ],
   "source": [
    "def check_nan(data):\n",
    "    for i in ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED']:\n",
    "            print('nan entries of %s is %s'%(i, data.loc[data[i].isnull()].shape[0]))\n",
    "    return \n",
    "check_nan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>NUMBER</th>\n",
       "      <th>ISO_TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>WMO_WIND</th>\n",
       "      <th>WMO_PRES</th>\n",
       "      <th>DIST2LAND</th>\n",
       "      <th>STORM_SPEED</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sign_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016005N02187</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 06:00:00</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>-173.500</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016005N02187</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 09:00:00</td>\n",
       "      <td>2.0450</td>\n",
       "      <td>-173.353</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016005N02187</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 12:00:00</td>\n",
       "      <td>2.1000</td>\n",
       "      <td>-173.200</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016005N02187</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 15:00:00</td>\n",
       "      <td>2.1775</td>\n",
       "      <td>-173.042</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1005.5</td>\n",
       "      <td>1759.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016005N02187</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 18:00:00</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>-172.900</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>1770.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SID NUMBER            ISO_TIME     LAT      LON  WMO_WIND  \\\n",
       "1  2016005N02187      1 2016-01-05 06:00:00  2.0000 -173.500      25.0   \n",
       "2  2016005N02187      1 2016-01-05 09:00:00  2.0450 -173.353      25.0   \n",
       "3  2016005N02187      1 2016-01-05 12:00:00  2.1000 -173.200      25.0   \n",
       "4  2016005N02187      1 2016-01-05 15:00:00  2.1775 -173.042      25.0   \n",
       "5  2016005N02187      1 2016-01-05 18:00:00  2.3000 -172.900      25.0   \n",
       "\n",
       "   WMO_PRES  DIST2LAND  STORM_SPEED   cos_day  sign_day  \n",
       "1    1006.0     1738.0          3.0  0.996298         1  \n",
       "2    1006.0     1737.0          3.0  0.996298         1  \n",
       "3    1006.0     1748.0          3.0  0.996298         0  \n",
       "4    1005.5     1759.0          4.0  0.996298         0  \n",
       "5    1005.0     1770.0          4.0  0.996298         0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df0.iloc[1:].copy()\n",
    "# pd.to_datetime(df['ISO_TIME'], format= '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def smooth_day(df):\n",
    "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], format= '%Y-%m-%d %H:%M:%S')\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['ISO_TIME'].dt.day / 365)\n",
    "    df['sign_day'] = 0 \n",
    "    df.loc[(df['ISO_TIME'].dt.hour <=11) & (df['ISO_TIME'].dt.hour >=0),'sign_day'] = 1\n",
    "    return df\n",
    "\n",
    "smooth_day(df).head()\n",
    "# df.loc[(df['ISO_TIME'].dt.hour <=11) & (df['ISO_TIME'].dt.hour >=0)]['SID']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to have one-hot encoding of basin and nature of the storm\n",
    "def add_one_hot(data, df0):\n",
    "    basin = pd.get_dummies(data['BASIN'],prefix='basin')\n",
    "    basin.drop(columns=['basin_ '], inplace = True)\n",
    "    nature = pd.get_dummies(data['NATURE'],prefix='nature')\n",
    "    nature.drop('nature_ ', axis=1, inplace = True)\n",
    "    frames = [df0, basin, nature]\n",
    "    df0 = pd.concat(frames, axis = 1)\n",
    "    print(\"Basin and Nature of the storm are now added and one-hot.\")\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code allows to get the maximum wind change in the last X hours.\n",
    "def get_max_change(data, time, i):\n",
    "    t = time//3\n",
    "    try:\n",
    "        val = max(data['WMO_WIND'][i-t:i])-min(data['WMO_WIND'][i-t:i])\n",
    "    except:\n",
    "        val = 'NaN'\n",
    "    return val\n",
    "\n",
    "#please specify a multiple of 3h for the time\n",
    "def get_max_wind_change(data, time):\n",
    "    df = data\n",
    "    df['max_wind_change']=[get_max_change(data, time, i) for i in range(len(data))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to use in the future: computes the wind category\n",
    "def sust_wind_to_cat_one_hot(wind):\n",
    "    # maximum sustained wind in kt (knot)\n",
    "    if wind<=33: cat='TD' # <=33\n",
    "    elif wind<=63.:  cat='TS'\n",
    "    elif wind <=82.: cat='H1'\n",
    "    elif wind <=95.: cat='H2'\n",
    "    elif wind <=112.: cat='H3'\n",
    "    elif wind <=136.: cat='H4'    \n",
    "    elif wind > 136. : cat='H5'\n",
    "    else: cat = 'nan'  \n",
    "\n",
    "    return cat\n",
    "\n",
    "def sust_wind_to_cat_val(wind):\n",
    "    # maximum sustained wind in kt (knot)\n",
    "    if wind<=33: cat= 0 # <=33\n",
    "    elif wind<=63.:  cat=1\n",
    "    elif wind <=82.: cat=2\n",
    "    elif wind <=95.: cat=3\n",
    "    elif wind <=112.: cat=4\n",
    "    elif wind <=136.: cat=5    \n",
    "    elif wind > 136. : cat=6\n",
    "    else: cat = 0  \n",
    "\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_storm_category_one_hot(data):\n",
    "    df = pd.DataFrame()\n",
    "    df['storm_category'] = [sust_wind_to_cat_one_hot(data['WMO_WIND'][i]) for i in range(len(data))]\n",
    "    storm_cat = pd.get_dummies(df['storm_category'],prefix='storm_category')\n",
    "    #storm_cat\n",
    "    storm_cat.drop('storm_category_nan', axis=1, inplace=True)\n",
    "    frames = [data, storm_cat]\n",
    "    df0 = pd.concat(frames, axis = 1)\n",
    "    #df0.drop('storm_category', axis=1)\n",
    "    print(\"Storm category is now added and one-hot.\")\n",
    "    return df0\n",
    "\n",
    "def add_storm_category_val(data):\n",
    "    df = pd.DataFrame()\n",
    "    df['storm_category'] = [sust_wind_to_cat_val(data['WMO_WIND'][i]) for i in range(len(data))]\n",
    "    frames = [data, df]\n",
    "    df0 = pd.concat(frames, axis = 1)\n",
    "    #df0.drop('storm_category', axis=1)\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_storm(data, min_wind, min_steps = 5, max_steps = 120):\n",
    "    '''function to create dictionary of storm matrices\n",
    "    arguments: \n",
    "    data we want to cut\n",
    "    min_wind: the minimum wind speed to store data \n",
    "    '''\n",
    "    #get unique storm_id:\n",
    "    SID=pd.unique(data['SID']).tolist()\n",
    "    #remove empty SID\n",
    "    #if not dropna: SID.remove(' ') \n",
    "    #create empty dictionary\n",
    "    dict0={}\n",
    "    ind = 0\n",
    "    for i in range(len(SID)):\n",
    "        #get data of a particular SID\n",
    "        M = data.loc[data['SID'] == SID[i]]\n",
    "        #cut off using min wind speed\n",
    "        #TODO : cut everything before, ie look for the right date\n",
    "        try:\n",
    "            t = M.index[M['WMO_WIND']>= min_wind][0]\n",
    "            t0 = M.index[0]\n",
    "        except:\n",
    "            t = 0\n",
    "        N = M.loc[M['WMO_WIND'] >= min_wind]\n",
    "        #save matrix in dict0\n",
    "        if N.shape[0] > min_steps:\n",
    "            ind+=1\n",
    "            dict0.update({ind:M.iloc[t-t0:max_steps+t-t0]})\n",
    "    print(\"The dictionary of storms has been created.\")\n",
    "    return dict0\n",
    "\n",
    "#storms is the dictionary all storm matrices\n",
    "#storms = sort_storm(df0, min_wind=45, min_steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Geographical difference features: i.e. feature_1(t) = feature(t)-feature(0)\n",
    "    # features: LAT, LON, DIST2LAND\n",
    "def geo_diff(dict0):\n",
    "    dict1={}\n",
    "    #loop over each dataframe\n",
    "    for i in dict0:\n",
    "        df=dict0[i]\n",
    "        #reset index\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        #calculate difference from t=0 \n",
    "        df['LAT_1']= df['LAT'] - df['LAT'][0]\n",
    "        df['LON_1']= df['LON'] - df['LON'][0]\n",
    "        df['DIST2LAND_1']= df['DIST2LAND'] - df['DIST2LAND'][0]\n",
    "        #substitute back to the dictionary\n",
    "        dict1[i]=df\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instead of padding with 0, pad with latest values in loop\n",
    "def pad_traj(dict0, max_steps, nan = False):\n",
    "    dict1={}\n",
    "    for t in dict0:\n",
    "        num_steps = dict0[t].shape[0]\n",
    "        steps2add = max_steps - num_steps\n",
    "        if steps2add > 0:\n",
    "            if nan:\n",
    "                dict1[t] = pd.concat([dict0[t], pd.DataFrame([[np.nan] * dict0[t].shape[1]]*steps2add, columns=dict0[t].columns)], ignore_index=True)\n",
    "            else:\n",
    "                dict1[t] = pd.concat([dict0[t], pd.DataFrame([[0] * dict0[t].shape[1]]*steps2add, columns=dict0[t].columns)], ignore_index=True)                \n",
    "                #In fact it happens to be easier to make the change afterwards with repad\n",
    "                #dict1[t] = pd.concat([dict0[t], pd.DataFrame([dict0[t].tail(1)]*steps2add, columns=dict0[t].columns)], ignore_index=True)                       \n",
    "        else:\n",
    "            dict1[t] = dict0[t][:max_steps]\n",
    "    print(\"The trajectories have now been padded.\")\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance_km(lon1, lat1, lon2, lat2):\n",
    "    '''\n",
    "    Using haversine formula (https://www.movable-type.co.uk/scripts/latlong.html)\n",
    "    '''\n",
    "    R=6371e3 # meters (earth's radius)\n",
    "    phi_1=math.radians(lat1)\n",
    "    phi_2 = math.radians(lat2)\n",
    "    delta_phi=math.radians(lat2-lat1)\n",
    "    delta_lambda=math.radians(lon2-lon1)\n",
    "    a=np.power(math.sin(delta_phi/2),2) + math.cos(phi_1)*math.cos(phi_2)\\\n",
    "      * np.power(math.sin(delta_lambda/2),2)\n",
    "    c= 2 * math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "\n",
    "    return R*c/1000.\n",
    "\n",
    "#compute the displacement from t=0\n",
    "def add_displacement_distance(dict0):\n",
    "    dict1={}\n",
    "    #loop over each dataframe\n",
    "    for i in dict0:\n",
    "        df=dict0[i]\n",
    "        #reset index\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        #calculate difference from t=0 \n",
    "        df['DISPLACEMENT'] = 0\n",
    "        for j in range(1,len(df)):\n",
    "            d = get_distance_km(df['LON'][j-1], df['LAT'][j-1], df['LON'][j], df['LAT'][j])\n",
    "            if d > 500: d=0\n",
    "            df['DISPLACEMENT'][j] = d \n",
    "        dict1[i]=df\n",
    "    return dict1\n",
    "\n",
    "def add_displacement_lat_lon2(dict0):\n",
    "    dict1={}\n",
    "    #loop over each dataframe\n",
    "    for i in dict0:\n",
    "        df=dict0[i]\n",
    "        #reset index\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        lst_lat = [0]\n",
    "        lst_lon = [0]\n",
    "        for j in range(1,len(df)):\n",
    "            d_lat = df['LAT'][j] - df['LAT'][j-1]\n",
    "            d_lon = df['LON'][j] - df['LON'][j-1]\n",
    "            lst_lat.append(d_lat)\n",
    "            lst_lon.append(d_lon)\n",
    "        df['DISPLACEMENT_LAT'] = lst_lat\n",
    "        df['DISPLACEMENT_LON'] = lst_lon\n",
    "        dict1[i]=df\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to calculate tensor shape\n",
    "    #input: dictionary of storm data\n",
    "def tensor_shape(dict0):\n",
    "    #number of storms\n",
    "    num_storms=len(dict0) - 1\n",
    "    #number of features\n",
    "    num_features=dict0[next(iter(dict0))].shape[1]  \n",
    "    \n",
    "    #to compute min and max number of steps\n",
    "    t_max = 0 #initialise \n",
    "    t_min = 1000\n",
    "    t_hist = []\n",
    "    for i in dict0:\n",
    "        t0 = dict0[i].shape[0]\n",
    "        t_hist.append(t0)\n",
    "        if  t0 > t_max:\n",
    "            t_max = t0\n",
    "        if t0 < t_min:\n",
    "            t_min = t0\n",
    "    print(\"There are %s storms with %s features, and maximum number of steps is %s and minimum is %s.\" %(num_storms,num_features,t_max, t_min))\n",
    "    return num_storms, num_features, t_max, t_min, t_hist     \n",
    "    \n",
    "#call tensor_shape \n",
    "#m, n, t_max, t_min, t_hist = tensor_shape(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a tensor\n",
    "def create_tensor(data, number_of_storms):\n",
    "    tensor = data[1]\n",
    "    for i in range(2,number_of_storms,1):\n",
    "        tensor=np.dstack((tensor, data[i]))\n",
    "    #return list of features \n",
    "    p_list = data[1].columns.tolist()\n",
    "    return tensor, p_list\n",
    "\n",
    "def repad(t):\n",
    "    for i in range(t.shape[0]):\n",
    "        if t[i][2][-1] == 0:\n",
    "            ind = np.argmin(t[i][2])\n",
    "            for j in range(ind,t.shape[2]):\n",
    "                t[i,:,j]=t[i,:,ind-1]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path = \"./data/ibtracs.last3years.list.v04r00.csv\", max_wind_change = 12, min_wind = 50, min_steps = 15, max_steps = 120, secondary = False, one_hot=False, dropna = False):\n",
    "    data = pd.read_csv(path) \n",
    "    #select interesting columns\n",
    "    df0 = select_data(data)\n",
    "    #transform data from String to numeric\n",
    "    df0 = numeric_data(df0)\n",
    "    #add cos & sign of the day\n",
    "    df0 = smooth_day(df0)\n",
    "    #if dropna: df0 = df0.dropna()\n",
    "    #add one_hot columns:\n",
    "    if one_hot: \n",
    "        #add one-hot storm category\n",
    "        #df0 = add_storm_category_val(df0)   \n",
    "        df0 = add_storm_category_one_hot(df0)\n",
    "        #transform basin and nature of the storm into one-hot vector\n",
    "        df0 = add_one_hot(data, df0)\n",
    "    if secondary: \n",
    "        #add the max-wind-change column\n",
    "        df0 = get_max_wind_change(df0, max_wind_change)\n",
    "    \n",
    "    #get a dict with the storms with a windspeed greater to a threshold\n",
    "    storms = sort_storm(df0, min_wind, min_steps)\n",
    "    #pad the trajectories to a fix length\n",
    "    d = pad_traj(storms, max_steps)\n",
    "    #print(d)\n",
    "    if secondary:\n",
    "        #d = add_displacement_distance(d)\n",
    "        d = add_displacement_lat_lon2(d)\n",
    "    #print the shape of the tensor\n",
    "    m, n, t_max, t_min, t_hist = tensor_shape(d)\n",
    "    #create the tensor\n",
    "    t, p_list = create_tensor(d, m)\n",
    "    #delete id and number of the storms\n",
    "    t2 = torch.Tensor(t[:,3:,:].astype('float64'))\n",
    "    #match feature list \n",
    "    p_list = p_list[3:]\n",
    "    #transpose time and sample\n",
    "    t3=torch.transpose(t2,0,2)\n",
    "    #replace 0 by latest values in the tensor\n",
    "    t3 = repad(t3)\n",
    "    return t3, p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data2(path = \"./data/ibtracs.last3years.list.v04r00.csv\", max_wind_change = 12, min_wind = 50, min_steps = 15, max_steps = 120, secondary = False, one_hot=False, dropna = False):\n",
    "    data = pd.read_csv(path) \n",
    "    #select interesting columns\n",
    "    df0 = select_data(data)\n",
    "    #transform data from String to numeric\n",
    "    df0 = numeric_data(df0)\n",
    "    #if dropna: df0 = df0.dropna()\n",
    "    #add one_hot columns:\n",
    "    if one_hot: \n",
    "        #add one-hot storm category\n",
    "        #df0 = add_storm_category_val(df0)   \n",
    "        df0 = add_storm_category_one_hot(df0)\n",
    "        #transform basin and nature of the storm into one-hot vector\n",
    "        df0 = add_one_hot(data, df0)\n",
    "    if secondary: \n",
    "        #add the max-wind-change column\n",
    "        df0 = get_max_wind_change(df0, max_wind_change)\n",
    "    \n",
    "    #get a dict with the storms with a windspeed greater to a threshold\n",
    "    storms = sort_storm(df0, min_wind, min_steps)\n",
    "    #pad the trajectories to a fix length\n",
    "    d = pad_traj(storms, max_steps)\n",
    "    #print(d)\n",
    "    if secondary:\n",
    "        #d = add_displacement_distance(d)\n",
    "        d = add_displacement_lat_lon2(d)\n",
    "    #print the shape of the tensor\n",
    "    m, n, t_max, t_min, t_hist = tensor_shape(d)\n",
    "    #create the tensor\n",
    "    t, p_list = create_tensor(d, m)\n",
    "    #delete id and number of the storms\n",
    "    #t2 = torch.Tensor(t[:,1:3,:])#.astype('float64'))\n",
    "    #match feature list \n",
    "    #p_list = p_list[3:]\n",
    "    #transpose time and sample\n",
    "    #t3=torch.transpose(t2,0,2)\n",
    "    #replace 0 by latest values in the tensor\n",
    "    #t3 = repad(t3)\n",
    "    return t[:,2:5,:], p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary of storms has been created.\n",
      "The trajectories have now been padded.\n",
      "There are 11 storms with 9 features, and maximum number of steps is 60 and minimum is 60.\n"
     ]
    }
   ],
   "source": [
    "t3, p_list = prepare_data2(min_wind = 60, min_steps = 60, max_steps=60, one_hot = False, secondary = False, dropna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary of storms has been created.\n",
      "The trajectories have now been padded.\n",
      "There are 1830 storms with 12 features, and maximum number of steps is 60 and minimum is 60.\n"
     ]
    }
   ],
   "source": [
    "t3, p_list = prepare_data(path = \"ibtracs.since1980.list.v04r00.csv\", min_wind = 60, min_steps = 1, max_steps=60, one_hot = False, secondary = True, dropna = True)\n",
    "#d = prepare_data2(min_wind = 120, min_steps = 1, max_steps=60, one_hot = False, secondary = True, dropna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 dimensional SVD \n",
    "def mat_svd(X,r,m,n,t):\n",
    "    #Mode-1 folding\n",
    "    X1= X.reshape(m,n*t)\n",
    "    U,S,V= torch.svd(X1,some=True, compute_uv=True) \n",
    "    U1=U[:,:r]\n",
    "    #Mode-2 folding\n",
    "    X2= X.reshape(m*t,n) \n",
    "    U,S,V= torch.svd(X2,some=True, compute_uv=True) \n",
    "    V1=V[:,:r]\n",
    "    return U1,V1\n",
    "\n",
    "#calculate MAE:\n",
    "def get_mae(tensor1,tensor2):\n",
    "    error = (tensor1-tensor2).abs().sum().item()\n",
    "    return error \n",
    "\n",
    "\n",
    "#Slice Iteration: for one iteration \n",
    "def slice_iteration(tensor,r,m,n,t):\n",
    "    new_tensor = torch.empty((m,n,t)) \n",
    "    #get U,V\n",
    "    U,V = mat_svd(tensor, r,m,n,t)\n",
    "    \n",
    "    #update tensor \n",
    "    UU = torch.mm(U,torch.t(U))\n",
    "    VV = torch.mm(V,torch.t(V))\n",
    "    for i in range(t):\n",
    "        new_tensor[:,:,i] = torch.mm(torch.mm(UU, tensor[:,:,i]),VV) \n",
    "    return new_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Slice Learning \n",
    "def slice_learning(tensor_train,r,max_iteration=50,convergence=0.001, plot=False):\n",
    "    m,n,t = tensor_train.shape\n",
    "    new_tensor = torch.empty(m,n,t) \n",
    "    old_tensor = tensor_train \n",
    "    mae_all=[]\n",
    "    \n",
    "    mae = convergence + 1 \n",
    "    #Iterative slice learning \n",
    "    for i in range(max_iteration): \n",
    "        if mae > convergence: \n",
    "            new_tensor = slice_iteration(old_tensor,r,m,n,t)\n",
    "            mae = get_mae(new_tensor,old_tensor) \n",
    "            mae_all.append(mae)\n",
    "            old_tensor = new_tensor \n",
    "    \n",
    "    #plot MAE difference between iterations   \n",
    "    max_i = len(mae_all)\n",
    "\n",
    "    print('Code stopped at iteration = %s, and mae difference = %s'%(max_i,mae_all[-1]))\n",
    "    if plot:\n",
    "        plt.plot(np.linspace(1,max_i,max_i),mae_all)\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "    return new_tensor \n",
    "\n",
    "#X_imputed = slice_learning(t3_test2,16)\n",
    "\n",
    "#find the MAE difference between X_imputed and X_original\n",
    "#print('The MAE between X_imputed and X_original is %s' %(get_mae(X_imputed,t3_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to standardize and de_standardize data \n",
    "def standardize(tensor): \n",
    "    new_tensor = tensor.clone()\n",
    "    #calculate along 2nd (python index) dimension \n",
    "    #std, mean = torch.std_mean(tensor[:,:k,:], dim=2)\n",
    "    # @cynthia: I replaced your line by this one so that it works with my torch version\n",
    "    std, mean = torch.std(tensor[:,:,:], dim=2), torch.mean(tensor[:,:,:], dim=2)\n",
    "    t=tensor.shape[2]\n",
    "    for i in range(t):\n",
    "        #element wise division\n",
    "        new_tensor[:,:,i] = torch.div((tensor[:,:,i] - mean),std)  \n",
    "    #replace Nan with 0 \n",
    "    new_tensor[new_tensor != new_tensor] = 0\n",
    "    return new_tensor, mean, std\n",
    "\n",
    "def de_standardize(tensor,mean,std):\n",
    "    t=tensor.shape[2]\n",
    "    new_tensor = tensor.clone()\n",
    "    for i in range(t):\n",
    "        #element wise multiplication\n",
    "        new_tensor[:,:,i] = torch.mul(tensor[:,:,i] ,std) + mean \n",
    "    return new_tensor \n",
    "\n",
    "# a,m,std=standardize(x)\n",
    "# b=de_standardize(a,m,std) \n",
    "# print('standardize',a )\n",
    "# print('de_standardize', b)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Methods to standardize and de_standardize data along samples and timesteps (full standardization)\n",
    "def standardize2(tensor): \n",
    "    new_tensor = tensor.clone()\n",
    "    #calculate along 0 and 2nd (python index) dimension \n",
    "    std, mean = torch.std(tensor, dim=[0,2]), torch.mean(tensor, dim=[0,2])\n",
    "    t = tensor.shape[1]\n",
    "    for i in range(t):\n",
    "        #element wise division\n",
    "        new_tensor[:,i,:] = torch.div((tensor[:,i,:] - mean[i]), std[i])  \n",
    "    #replace Nan with 0 \n",
    "    new_tensor[new_tensor != new_tensor] = 0\n",
    "    return new_tensor, mean, std\n",
    "\n",
    "def de_standardize2(tensor, mean, std):\n",
    "    t = tensor.shape[1]\n",
    "    new_tensor = tensor.clone()\n",
    "    for i in range(t):\n",
    "        #element wise multiplication\n",
    "        new_tensor[:,i,:] = torch.mul(tensor[:,i,:], std[i]) + mean[i] \n",
    "    return new_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot error bar chart\n",
    "def plot_error(data, X_imputed, storm_id, p_list):#calculate error for all features for next 5 hour prediction: \n",
    "    error_5=[]\n",
    "    error_10=[]\n",
    "    error_15=[]\n",
    "    for p in range(len(p_list)):\n",
    "        #for each time\n",
    "        error_v=np.abs(X_imputed[storm_id][p] - data[storm_id][p])\n",
    "        error_5.append((error_v[time+5-1:time+5+1].sum()*(1/3)))#avg of time +5\n",
    "        error_10.append((error_v[time+10-1:time+10+1].sum()*(1/3)))#avg of time +10\n",
    "        error_15.append((error_v[time+15-1:time+15+1].sum()*(1/3)))#avg of time +15\n",
    "    \n",
    "    #plotting error:\n",
    "    ind = np.arange(len(p_list)) \n",
    "    width = 0.3 \n",
    "    plt.bar(ind,error_5,width, label='t+5')\n",
    "    plt.bar(ind+width,error_10,width, label='t+10')\n",
    "    plt.bar(ind+2*width,error_15,width, label='t+15')\n",
    "    plt.xticks(ind + width / 2, p_list)\n",
    "    plt.title('Avg error for t+5/10/15 StormID=%s' %( storm_id))\n",
    "    plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "def plot_feature_prediction(data, X_imputed_0, X_imputed_1, X_imputed_2, storm_id, p_list, p_range):\n",
    "    for p in p_range:\n",
    "        #plot predicted feature p with various normalization methods: \n",
    "        l = [i for i in range(data.shape[2])]\n",
    "        plt.plot(l, np.array(X_imputed_0[storm_id][p]), label='imputed_no_scale')\n",
    "        plt.plot(l, np.array(X_imputed_1[storm_id][p]),label='imputed_normalize')\n",
    "        plt.plot(l, np.array(X_imputed_2[storm_id][p]), label='imputed_standardize')\n",
    "        plt.plot(l, np.array(data[storm_id][p]),label='real')\n",
    "        plt.axvline(x=time)\n",
    "        plt.legend()\n",
    "        plt.title('Feature= %s, StormID=%s' %(p_list[p],storm_id))\n",
    "        plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing function by deleting entries after time t for one particular storm  \n",
    "def test(data, p_list, storm_id, time, r, p = 2, delete = True):\n",
    "    data0 = data.clone()\n",
    "    \n",
    "    #data0= delete one storm data for testing  \n",
    "    if delete:    \n",
    "        for t in range(time,data.shape[2]):\n",
    "            data0[storm_id,:,t] = data0[storm_id,:,time-1]\n",
    "    \n",
    "    ### slice_learning 0: no_scale \n",
    "    X_imputed_0 = slice_learning(data0,r)\n",
    "    \n",
    "    ### slice learning 1: normalize data \n",
    "    data1 = F.normalize(data0, dim = 2)\n",
    "    X_imputed_1 = slice_learning(data1,r)\n",
    "    #de_normalize\n",
    "    scale = torch.div(data[:,:,:time],X_imputed_1[:,:,:time])\n",
    "    scale_mean = torch.mean(scale, dim=2)\n",
    "    for i in range(X_imputed_1.shape[2]):\n",
    "        X_imputed_1[:,:,i] = torch.mul(X_imputed_1[:,:,i], scale_mean) \n",
    "    \n",
    "    ### slice_learning 2: standardize\n",
    "    data2, mean, std = standardize2(data0)\n",
    "    X_imputed_2 = slice_learning(data2, r)\n",
    "    #de_standardize \n",
    "    X_imputed_2= de_standardize2(X_imputed_2, mean, std)\n",
    "    \n",
    "    return X_imputed_0, X_imputed_1, X_imputed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary of storms has been created.\n",
      "The trajectories have now been padded.\n",
      "There are 344 storms with 9 features, and maximum number of steps is 60 and minimum is 60.\n"
     ]
    }
   ],
   "source": [
    "t3, p_list = prepare_data(path = \"ibtracs.since1980.list.v04r00.csv\", min_wind = 50, min_steps= 59, max_steps=60, one_hot = False, secondary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60 is big storm not easy to predict\n",
    "storm_id = 340\n",
    "time = 20\n",
    "r = 13 #size of latent features \n",
    "for r in [11]:\n",
    "    print ('r =', r)\n",
    "    #test learning for a particular storm_id \n",
    "    X_imputed_0, X_imputed_1, X_imputed_2 = test(t3, p_list, storm_id, time, r, delete = False)\n",
    "    #make some plots:\n",
    "    plot_error(t3, X_imputed_2, storm_id, p_list)\n",
    "    plot_feature_prediction(t3, X_imputed_0, X_imputed_1, X_imputed_2, storm_id, p_list, p_range=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best r\n",
    "def get_best_r(data, r_min, r_max):\n",
    "    r_list = []\n",
    "    for r in range(r_min, r_max):\n",
    "        X_imputed = slice_learning(data,r, plot = False)\n",
    "        r_list.append(get_mae(X_imputed,data))\n",
    "    plt.plot([r for r in range(r_min, r_max)], r_list)\n",
    "    plt.show()\n",
    "    return r_list\n",
    "\n",
    "get_best_r(t3[:,2:22,:], 5, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=5\n",
    "m,n,t=t3.shape\n",
    "\n",
    "#SVD \n",
    "def mat_svd(X,r):\n",
    "    #Mode-1 folding\n",
    "    X1= X.reshape(m,n*t)\n",
    "    U,S,V= torch.svd(X1,some=True, compute_uv=True) \n",
    "    U1=U[:,:r]\n",
    "    #Mode-2 folding\n",
    "    X2= X.reshape(m*t,n) \n",
    "    U,S,V= torch.svd(X2,some=True, compute_uv=True)\n",
    "    V1=V[:,:r]\n",
    "    return U1,V1\n",
    "\n",
    "#Update S based on U1, V1\n",
    "def update_s(U, X, V, r ):\n",
    "    new_s = torch.empty((r,r,t))\n",
    "    for i in range(t): \n",
    "        m1 = torch.mm(torch.t(U),X[:,:,i]) #torch.t = transpose\n",
    "        m2 = torch.mm(m1,V)\n",
    "        new_s[:,:,i]=m2\n",
    "    return new_s\n",
    "\n",
    "#Compute Z:\n",
    "def update_z(U,V,S):\n",
    "    new_z = torch.empty((m,n,t))\n",
    "    for i in range(t): \n",
    "        S_t=S[:,:,i]\n",
    "        m1 = torch.mm(U,S_t)\n",
    "        m2 = torch.mm(m1,torch.t(V))\n",
    "        new_z[:,:,i]=m2\n",
    "    return new_z\n",
    "\n",
    "#wrap up everything \n",
    "def tensor_svd1(Z0,r,T=50):\n",
    "    \n",
    "    #initialise R\n",
    "    R=torch.rand(Z0.shape)\n",
    "    error=100\n",
    "    error_all=[]\n",
    "    \n",
    "    for t in range(T):\n",
    "        if error > 1: \n",
    "            U1,V1= mat_svd(R,r)\n",
    "            S=update_s(U1,R,V1,r)\n",
    "            Z1=update_z(U1,V1,S)\n",
    "            R=Z1\n",
    "            #calculate error \n",
    "            error=(Z1-Z0).pow(2).sum().item()\n",
    "            error_all.append(error)\n",
    "    \n",
    "    print('code stopped at t=%s at error=%s'%(t,error))\n",
    "    \n",
    "    #plot error \n",
    "    L=len(error_all)\n",
    "    plt.plot(np.linspace(1,L,L),error_all)\n",
    "    plt.show()\n",
    "    \n",
    "    return Z1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_svd1(t3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "print('x=',x)\n",
    "#     m,n,t = x.shape\n",
    "#     mean = torch.zeros(m,n)\n",
    "#     std = torch.zeros(m,n)\n",
    "#     for i in range(m):\n",
    "#     for j in range(n):\n",
    "#         mean[i,j] = x[i,j,:].mean()\n",
    "#         std[i,j] = x[i,j,:].std() \n",
    "#     print ('loop result',std,mean)\n",
    "\n",
    "#calculate standard deviation and mean of tensor\n",
    "def standardize(tensor):\n",
    "    new_tensor = tensor.clone()\n",
    "    #calculate along 2nd (python index) dimension \n",
    "    std, mean = torch.std_mean(tensor, dim=2)\n",
    "    t=tensor.shape[2]\n",
    "    for i in range(t):\n",
    "        #element wise division\n",
    "        new_tensor[:,:,i] = torch.div((tensor[:,:,i] - mean),std) \n",
    "    return new_tensor, mean, std\n",
    "\n",
    "def de_standardize(tensor,mean,std):\n",
    "    t=tensor.shape[2]\n",
    "    new_tensor = tensor.clone()\n",
    "    for i in range(t):\n",
    "        #element wise multiplication\n",
    "        new_tensor[:,:,i] = torch.mul(tensor[:,:,i] ,std) + mean \n",
    "    return new_tensor \n",
    "a,m,std=standardize(x)\n",
    "b=de_standardize(a,m,std) \n",
    "print('standardize',a )\n",
    "print('de_standardize', b)\n",
    " \n",
    "    \n",
    "torch.svd(a,some=True, compute_uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to use in the future: computes the wind category\n",
    "def sust_wind_to_cat(wind):\n",
    "    # maximum sustained wind in kt (knot)\n",
    "    if wind<=33: cat='TD' # <=33\n",
    "    elif wind<=63.:  cat='TS'\n",
    "    elif wind <=82.: cat='H1'\n",
    "    elif wind <=95.: cat='H2'\n",
    "    elif wind <=112.: cat='H3'\n",
    "    elif wind <=136.: cat='H4'\n",
    "    else: cat='H5'\n",
    "\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allows to keep only the data between two dates to specify\n",
    "def get_periodic_data(data, start ='2018-01-01',end ='2018-12-31',mincat=None,maxcat=None):\n",
    "    n = len(data)\n",
    "    start = datetime.strptime(start, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end, '%Y-%m-%d')\n",
    "    times = pd.to_datetime(data['ISO_TIME'][1:n])\n",
    "    ind_start = 1\n",
    "    ind_end = n\n",
    "    i = 1\n",
    "    while times[i] < start :#or i < n:\n",
    "        i+=1\n",
    "    ind_start = i\n",
    "    while times[i] < end :#or i < n:\n",
    "        i+=1\n",
    "    ind_end = i\n",
    "    return data[ind_start:ind_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allows to get the distinct ids of the storms \n",
    "def get_ids(data):\n",
    "    return data.SID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_storm_track(data, storm_id = '2016005N02187', time_step = 6):\n",
    "    #data.set_index(\"SID\", inplace=True)\n",
    "    return data.loc[data['SID'] == storm_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storm = get_storm_track(data)\n",
    "lat = np.array(storm['LAT'])\n",
    "lat2 = [float(lat[i]) for i in range(len(lat))]\n",
    "lon = np.array(storm['LON'])\n",
    "lon2 = [float(lon[i]) for i in range(len(lon))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lat2, lon2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions we may use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance_km(lon1, lat1, lon2, lat2):\n",
    "    '''\n",
    "    Using haversine formula (https://www.movable-type.co.uk/scripts/latlong.html)\n",
    "    '''\n",
    "    R=6371e3 # meters (earth's radius)\n",
    "    phi_1=math.radians(lat1)\n",
    "    phi_2 = math.radians(lat2)\n",
    "    delta_phi=math.radians(lat2-lat1)\n",
    "    delta_lambda=math.radians(lon2-lon1)\n",
    "    a=np.power(math.sin(delta_phi/2),2) + math.cos(phi_1)*math.cos(phi_2)\\\n",
    "      * np.power(math.sin(delta_lambda/2),2)\n",
    "    c= 2 * math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "\n",
    "    return R*c/1000.\n",
    "\n",
    "\n",
    "def get_longlat_from_offsets(lon, lat, dkm_lon, dkm_lat):\n",
    "    '''\n",
    "    :param lon: initial longitude\n",
    "    :param lat: inital latitude\n",
    "    :param dn:     offsets in meters\n",
    "    :param de:     offsets in meters\n",
    "    :return: lon_final, lat_final\n",
    "    '''\n",
    "    # Earthâ€™s radius, sphere\n",
    "    R = 6378137\n",
    "    # Coordinate offsets in radians\n",
    "    dLat = dkm_lat / R\n",
    "    dLon = dkm_lon / (R * math.cos(math.radians(lat)))\n",
    "\n",
    "    # OffsetPosition, decimal degrees\n",
    "    latO = lat + dLat * 180 / math.pi\n",
    "    lonO = lon + dLon * 180 / math.pi\n",
    "\n",
    "    return lonO, latO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
