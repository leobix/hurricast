{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from run import Prepro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "# import tensorly as tl #tensorly package\n",
    "# from tensorly.decomposition import tucker #tucker decomp package\n",
    "import utils_tensor as utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_intensity_accu</th>\n",
       "      <th>compression_error</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>past_n_steps</th>\n",
       "      <th>pred_n_steps</th>\n",
       "      <th>xgb_intensity_accu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.797</td>\n",
       "      <td>113.513023</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   base_intensity_accu  compression_error  max_depth  past_n_steps  \\\n",
       "0                0.797         113.513023          5             8   \n",
       "\n",
       "   pred_n_steps  xgb_intensity_accu  \n",
       "0             2               0.813  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../cluster_results/xgb_tensor_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "#vision_data = np.load('data/vision_data_30_16_120_3years_test2.npy', allow_pickle = True)\n",
    "vision_data = np.load('../data/vision_data_50_20_60_3years_v2.npy', allow_pickle = True)\n",
    "# vision_data = np.load('../../../Volumes/Samsung_T5/vision_data_50_20_90_1980_v3.npy', allow_pickle = True)\n",
    "#y = np.load('data/y_30_16_120_3years_test2.npy', allow_pickle = True)\n",
    "y = np.load('../data/y_50_20_60_3years_v2.npy', allow_pickle = True) \n",
    "# y = np.load('../../../Volumes/Samsung_T5/y_50_20_90_1980_v3.npy', allow_pickle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 60, 3, 3, 25, 25)\n",
      "(83, 60, 11)\n"
     ]
    }
   ],
   "source": [
    "print(vision_data.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset and corresponding sizes (null elements included):\n",
      "X_vision torch.Size([3071, 16, 9, 25, 25])\n",
      "X_stat torch.Size([3071, 16, 10])\n",
      "target_displacement torch.Size([3071, 8, 2])\n",
      "target_intensity torch.Size([3071])\n",
      "target_intensity_cat torch.Size([3071])\n",
      "target_intensity_cat_baseline torch.Size([3071])\n",
      "Keeping 2481 samples out of the initial 3071.\n",
      "Reshaping the displacement target...\n"
     ]
    }
   ],
   "source": [
    "train_test_split = 0.8 #how much train test data\n",
    "predict_at = 8 #steps_out\n",
    "window_size = 16 #how many timesteps from the past to take ie steps_in\n",
    "\n",
    "train_tensors, test_tensors = Prepro.process(vision_data, y, train_test_split, predict_at = predict_at, window_size=window_size)\n",
    "x_viz_train, x_stat_train, tgt_intensity_cat_train, tgt_intensity_cat_baseline_train, tgt_displacement_train, tgt_intensity_train = train_tensors\n",
    "x_viz_test, x_stat_test, tgt_intensity_cat_test, tgt_intensity_cat_baseline_test, tgt_displacement_test, tgt_intensity_test = test_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 60, 3, 3, 25, 25)\n",
      "torch.Size([1984, 16, 10])\n",
      "torch.Size([1984, 16, 9, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "print(vision_data.shape)\n",
    "print(x_stat_train.shape)\n",
    "print(x_viz_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.get_norms_4d(x_viz_train.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function intaking x_stat, x_viz and concat them according to ranks \n",
    "def concat_stat_viz(x_stat_train, x_viz_train, reduced_ranks): #third arg is vision tensor's dimensions \n",
    "    #reshape tabular data \n",
    "    X_train = x_stat_train.reshape(x_stat_train.shape[0], -1)\n",
    "\n",
    "    #compress vision data according to reduced_ranks \n",
    "    viz = np.zeros((x_viz_train.shape[0], np.prod(reduced_ranks)))\n",
    "    avg_error = 0 \n",
    "    for i in range(x_viz_train.shape[0]):\n",
    "        viz[i,:], error = utils.viz_to_arr(x_viz_train[i].numpy(), reduced_ranks)\n",
    "        avg_error += error \n",
    "    X_train = np.concatenate((X_train.numpy(), viz), axis=1)\n",
    "    compression_error = avg_error/x_viz_train.shape[0]\n",
    "    \n",
    "    return X_train, compression_error \n",
    "\n",
    "#concat viz with tabular \n",
    "X_train, error =utils.concat_stat_viz(x_stat_train, x_viz_train, reduced_ranks=[10,7,10,10])\n",
    "X_test, error  =concat_stat_viz(x_stat_test, x_viz_test, reduced_ranks=[10,7,10,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize x:\n",
    "def standardize_x(x_viz_train, x_viz_test, x_stat_train, x_stat_test):\n",
    "    means = x_viz_train.mean(dim=(0, 1, 3, 4))\n",
    "    stds = x_viz_train.std(dim=(0, 1, 3, 4))\n",
    "\n",
    "    means_stat = x_stat_train.mean(dim=(0, 1))\n",
    "    stds_stat = x_stat_train.std(dim=(0, 1))\n",
    "\n",
    "    for i in range(len(means)):\n",
    "        x_viz_train[:, :, i] = (x_viz_train[:, :, i] - means[i]) / stds[i]\n",
    "        x_viz_test[:, :, i] = (x_viz_test[:, :, i] - means[i]) / stds[i]\n",
    "\n",
    "    for i in range(len(means_stat)):\n",
    "        x_stat_train[:, :, i] = (x_stat_train[:, :, i] - means_stat[i]) / stds_stat[i]\n",
    "        x_stat_test[:, :, i] = (x_stat_test[:, :, i] - means_stat[i]) / stds_stat[i]\n",
    "    return x_viz_train, x_viz_test, x_stat_train, x_stat_test\n",
    "\n",
    "#standardize y: \n",
    "def standardize_y(y_train, y_test):\n",
    "    y_train = y_train.numpy()\n",
    "    y_test = y_test.numpy()\n",
    "    mean = y_train.mean()\n",
    "    std = y_train.std()\n",
    "    y_train = (y_train-mean)/std\n",
    "    y_test = (y_test-mean)/std\n",
    "    return y_train, y_test, mean , std \n",
    "\n",
    "x_viz_train, x_viz_test, x_stat_train, x_stat_test = standardize_x(x_viz_train, x_viz_test, x_stat_train, x_stat_test)\n",
    "tgt_intensity_train,tgt_intensity_test, mean_intensity, std_intensity  = standardize_y(tgt_intensity_train,tgt_intensity_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.021875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #get training and test tensors\n",
    "# ##maybe we don't have to call this for each prediction period, since x is the same, just y different? \n",
    "# # train_tensors, test_tensors = Prepro.process(vision_data, y, train_test_split, predict_at = predict_at, window_size=window_size)\n",
    "# # x_viz_train, x_stat_train, tgt_intensity_cat_train, tgt_intensity_cat_baseline_train, tgt_displacement_train, tgt_intensity_train = train_tensors\n",
    "# # x_viz_test, x_stat_test, tgt_intensity_cat_test, tgt_intensity_cat_baseline_test, tgt_displacement_test, tgt_intensity_test = test_tensors\n",
    "\n",
    "#need to standardize \n",
    "\n",
    "# # #concat viz with tabular \n",
    "# # X_train, compress_error =concat_stat_viz(x_stat_train, x_viz_train, reduced_ranks)\n",
    "# # X_test, _ =concat_stat_viz(x_stat_test, x_viz_test, reduced_ranks)\n",
    "\n",
    "# #run XGB on intensity\n",
    "xgb = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "xgb.fit(X_train, tgt_intensity_train)\n",
    "yhat = xgb.predict(X_test)\n",
    "mae_intensity = mean_absolute_error(tgt_intensity_test*std_intensity + mean_intensity, yhat*std_intensity+mean_intensity) \n",
    "\n",
    "\n",
    "\n",
    "# # xgb = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "# # xgb.fit(X_train, tgt_displacement_train[:,0])\n",
    "# # yhat = xgb.predict(X_test)\n",
    "# # mae_dx = mean_absolute_error(tgt_displacement_test[:,0], yhat) \n",
    "# # mae_dx\n",
    "\n",
    "# mae_intensity\n",
    "mae_intensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(vision_data, stat_data, window_size, predict_at, max_depth, reduced_ranks):\n",
    "    #get training and test tensors\n",
    "    ##maybe we don't have to call this for each prediction period, since x is the same, just y different?\n",
    "    train_test_split=0.8\n",
    "    train_tensors, test_tensors = Prepro.process(vision_data, stat_data, train_test_split, predict_at = predict_at, window_size=window_size)\n",
    "    x_viz_train, x_stat_train, tgt_intensity_cat_train, tgt_intensity_cat_baseline_train, tgt_displacement_train, tgt_intensity_train = train_tensors\n",
    "    x_viz_test, x_stat_test, tgt_intensity_cat_test, tgt_intensity_cat_baseline_test, tgt_displacement_test, tgt_intensity_test = test_tensors\n",
    "\n",
    "    #standardize:\n",
    "    means = x_viz_train.mean(dim=(0, 1, 3, 4))\n",
    "    stds = x_viz_train.std(dim=(0, 1, 3, 4))\n",
    "\n",
    "    means_stat = x_stat_train.mean(dim=(0, 1))\n",
    "    stds_stat = x_stat_train.std(dim=(0, 1))\n",
    "\n",
    "    for i in range(len(means)):\n",
    "        x_viz_train[:, :, i] = (x_viz_train[:, :, i] - means[i]) / stds[i]\n",
    "        x_viz_test[:, :, i] = (x_viz_test[:, :, i] - means[i]) / stds[i]\n",
    "\n",
    "    for i in range(len(means_stat)):\n",
    "        x_stat_train[:, :, i] = (x_stat_train[:, :, i] - means_stat[i]) / stds_stat[i]\n",
    "        x_stat_test[:, :, i] = (x_stat_test[:, :, i] - means_stat[i]) / stds_stat[i]\n",
    "\n",
    "    #concat viz with tabular\n",
    "    X_train, compress_error =utils.concat_stat_viz(x_stat_train, x_viz_train, reduced_ranks)\n",
    "    X_test, _ =utils.concat_stat_viz(x_stat_test, x_viz_test, reduced_ranks)\n",
    "\n",
    "    #run XGB on intensity\n",
    "    xgb = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "    xgb.fit(X_train, tgt_intensity_train)\n",
    "    yhat = xgb.predict(X_test)\n",
    "    mae_intensity = mean_absolute_error(tgt_intensity_test, yhat)\n",
    "    \n",
    "    mean_intensity = tgt_intensity_train.mean()\n",
    "    std_intensity = tgt_intensity_train.std()\n",
    "\n",
    "    #run XGB on displacement\n",
    "    xgb = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "    xgb.fit(X_train, tgt_displacement_train[:,0])\n",
    "    yhat = xgb.predict(X_test)\n",
    "    mae_dx = mean_absolute_error(tgt_displacement_test[:,0], yhat)\n",
    "\n",
    "    xgb = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "    xgb.fit(X_train, tgt_displacement_train[:,1])\n",
    "    yhat = xgb.predict(X_test)\n",
    "    mae_dy = mean_absolute_error(tgt_displacement_test[:,1], yhat)\n",
    "\n",
    "    # #run XGB on intensity category\n",
    "    xgb = XGBClassifier(max_depth=max_depth, n_estimators=80)\n",
    "    xgb.fit(X_train, tgt_intensity_cat_train)\n",
    "    yhat = xgb.predict(X_test)\n",
    "    score_xgb = accuracy_score(tgt_intensity_cat_test, yhat)\n",
    "    score_base = accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test)\n",
    "\n",
    "    return np.round(mae_intensity,3) , np.round(mae_dx,3), np.round(mae_dy,3), np.round(,3), np.round(score_xgb,3), np.round(score_base,3), compress_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2,20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset and corresponding sizes (null elements included):\n",
      "X_vision torch.Size([4233, 8, 9, 25, 25])\n",
      "X_stat torch.Size([4233, 8, 10])\n",
      "target_displacement torch.Size([4233, 2, 2])\n",
      "target_intensity torch.Size([4233])\n",
      "target_intensity_cat torch.Size([4233])\n",
      "target_intensity_cat_baseline torch.Size([4233])\n",
      "Keeping 3641 samples out of the initial 4233.\n",
      "Reshaping the displacement target...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6298e2226b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrun_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6298e2226b06>\u001b[0m in \u001b[0;36mrun_xgb\u001b[0;34m(window_size, predict_at, reduced_ranks)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#concat viz with tabular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress_error\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mconcat_stat_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_stat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_viz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mconcat_stat_viz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_stat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_viz_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4afa779b4be1>\u001b[0m in \u001b[0;36mconcat_stat_viz\u001b[0;34m(x_stat_train, x_viz_train, reduced_ranks)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mavg_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_viz_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mviz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz_to_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_viz_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mavg_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print('average tensor approx pct error is', avg_error/x_viz_train.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hurricast/utils_tensor.py\u001b[0m in \u001b[0;36mviz_to_arr\u001b[0;34m(tensor, reduced_ranks)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviz_to_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#low rank decomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtucker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_ranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mout_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mtucker\u001b[0;34m(tensor, rank, ranks, n_iter_max, init, svd, tol, random_state, verbose)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mmodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     return partial_tucker(tensor, modes, rank=rank, ranks=ranks, n_iter_max=n_iter_max, init=init,\n\u001b[0;32m--> 154\u001b[0;31m                           svd=svd, tol=tol, random_state=random_state, verbose=verbose)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mpartial_tucker\u001b[0;34m(tensor, modes, rank, n_iter_max, init, tol, svd, random_state, verbose, ranks)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mcore_approximation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_mode_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0meigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_approximation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigenvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorly/backend/core.py\u001b[0m in \u001b[0;36mpartial_svd\u001b[0;34m(self, matrix, n_eigenvecs, random_state, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m                 \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finfo_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_xgb(window_size, predict_at, reduced_ranks = [5,7,10,10]):\n",
    "    #get training and test tensors\n",
    "    ##maybe we don't have to call this for each prediction period, since x is the same, just y different? \n",
    "    train_tensors, test_tensors = Prepro.process(vision_data, y, train_test_split, predict_at = predict_at, window_size=window_size)\n",
    "    x_viz_train, x_stat_train, tgt_intensity_cat_train, tgt_intensity_cat_baseline_train, tgt_displacement_train, tgt_intensity_train = train_tensors\n",
    "    x_viz_test, x_stat_test, tgt_intensity_cat_test, tgt_intensity_cat_baseline_test, tgt_displacement_test, tgt_intensity_test = test_tensors\n",
    "    \n",
    "    #concat viz with tabular \n",
    "    X_train, compress_error =concat_stat_viz(x_stat_train, x_viz_train, reduced_ranks)\n",
    "    X_test, _ =concat_stat_viz(x_stat_test, x_viz_test, reduced_ranks)\n",
    "    \n",
    "    #run XGB on intensity\n",
    "    xgb = XGBRegressor(max_depth=max_depth, n_estimators=80)\n",
    "    xgb.fit(X_train, tgt_intensity_train)\n",
    "    yhat = xgb.predict(X_test)\n",
    "    score_xgb = mean_absolute_error(tgt_intensity_test, yhat)    \n",
    "    \n",
    "    #run XGB on displacement \n",
    "    xgb2 = XGBRegressor(max_depth=5, n_estimators=80)\n",
    "    xgb2.fit(X_train, tgt_displacement_train)\n",
    "    yhat2 = xgb.predict(X_test)\n",
    "    disp_xgb = mean_absolute_error(tgt_displacement_test, yhat2) \n",
    "    disp_base = mean_absolute_error(tgt_intensity_cat_test, tgt_displacement_baseline_test)\n",
    "    \n",
    "    return intensity_xgb, intensity_base, disp_xgb, disp_base, compress_error \n",
    "\n",
    "    \n",
    "run_xgb(window_size=8, predict_at=2, reduced_ranks = [5,7,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # #vision data to array data using tensor decomp \n",
    "# # def viz_to_arr(tensor, reduced_ranks):\n",
    "# #     #low rank decomp \n",
    "# #     core, factors = tucker(tensor, reduced_ranks)\n",
    "# #     out_arr = core.flatten()\n",
    "    \n",
    "# #     #calculate approximation error\n",
    "# #     approx = tucker_to_tensor(core, factors)\n",
    "# #     approx_error = tl.norm(approx-tensor)/tl.norm(tensor)*100  #euclidean norm \n",
    "    \n",
    "# #     return out_arr,  approx_error\n",
    "\n",
    "# out_arr, error = viz_to_arr(x_viz_train[50].numpy(), [5,7,10,10])\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.8000e+00, -1.7200e+02,  5.5000e+01,  9.9400e+02,  2.1470e+03,\n",
       "         8.0000e+00,  3.2100e+02,  1.0000e+00,  4.1247e-01, -3.0800e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stat_train[0,1,:]\n",
    "#'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'STORM_DIR', \n",
    "\n",
    "\n",
    "#'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND',\n",
    "#'STORM_SPEED', 'STORM_DIR', 'storm_category', 'basin_EP', 'basin_NI',\n",
    "#'basin_SI', 'basin_SP', 'basin_WP', 'nature_DS', 'nature_ET',\n",
    "#'nature_MX', 'nature_NR', 'nature_SS', 'nature_TS'\n",
    "#'STORM CATEGORY', \n",
    "#'STORM_DISPLACEMENT_X', 'STORM_DISPLACEMENT_Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2514, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_displacement_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2514, 8, 9, 25, 25])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_viz_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c4a648320>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_viz_train[20,7,5,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl #tensorly package\n",
    "from tensorly.decomposition import tucker #tucker decomp package\n",
    "# import hosvd as hosvd\n",
    "test_tensor = x_viz_train[30].numpy()\n",
    "# print(test_tensor.shape)\n",
    "\n",
    "# core, factors = tucker(test_tensor)\n",
    "# print(core.shape)\n",
    "# norms = get_norms_4d(test_tensor)\n",
    "# print((norms.cumsum()/norms.sum()).head(10)) #'cumsum of norms'\n",
    "\n",
    "# #try with rank [5,5,5]\n",
    "# core, factors = tucker(test_tensor, [5,5,5,5])\n",
    "# core.flatten()\n",
    "\n",
    "# #reconstruction accuracy: \n",
    "# approx = tucker_to_tensor(core, factors)\n",
    "# # print(hosvd.error(test_tensor, approx)) \n",
    "\n",
    "#function to calculate dimensional norms  \n",
    "def get_norms_4d(tensor):   \n",
    "  \n",
    "    tensor_shape = tensor.shape\n",
    "    dim = len(tensor.shape) \n",
    "    \n",
    "    core, _ = tucker(tensor, ranks=tensor_shape)\n",
    "     \n",
    "    norm0=[]\n",
    "    for i in range(core.shape[0]):\n",
    "        norm0.append(tl.norm(core[i,:,:,:]))  \n",
    "\n",
    "    norm1=[]\n",
    "    for i in range(core.shape[1]):\n",
    "        norm1.append(tl.norm(core[:,i,:,:]))  \n",
    "\n",
    "    norm2=[]\n",
    "    for i in range(core.shape[2]):\n",
    "        norm2.append(tl.norm(core[:,:,i,:]))  \n",
    "    \n",
    "    norm3 = [] \n",
    "    for i in range(core.shape[2]):\n",
    "        norm3.append(tl.norm(core[:,:,:,i])) \n",
    "    \n",
    "    norms = pd.concat([pd.Series(norm0),pd.Series(norm1),pd.Series(norm2),pd.Series(norm3)], axis=1) \n",
    "    norms_cumsum = norms.cumsum()/norms.sum()#'cumsum of norms'\n",
    "    return norms_cumsum \n",
    "\n",
    "#function reconstruct tensor given core and factors\n",
    "def tucker_to_tensor(core, factors):\n",
    "    tensor = core.copy()\n",
    "    dim = len(core.shape)\n",
    "    for i in range(dim):\n",
    "        tensor = tl.tenalg.mode_dot(tensor, factors[i], mode=i)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.672566920518875\n"
     ]
    }
   ],
   "source": [
    "#vision data to array data using tensor decomp \n",
    "def viz_to_arr(tensor, reduced_ranks):\n",
    "    #low rank decomp \n",
    "    core, factors = tucker(tensor, reduced_ranks)\n",
    "    out_arr = core.flatten()\n",
    "    \n",
    "    #calculate approximation error\n",
    "    approx = tucker_to_tensor(core, factors)\n",
    "    approx_error = tl.norm(approx-tensor)/tl.norm(tensor)*100  #euclidean norm \n",
    "    \n",
    "    return out_arr,  approx_error\n",
    "\n",
    "out_arr, error = viz_to_arr(x_viz_train[50].numpy(), [5,7,10,10])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768218</td>\n",
       "      <td>0.486046</td>\n",
       "      <td>0.485992</td>\n",
       "      <td>0.531203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.629811</td>\n",
       "      <td>0.643634</td>\n",
       "      <td>0.671907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.908902</td>\n",
       "      <td>0.762647</td>\n",
       "      <td>0.740847</td>\n",
       "      <td>0.752590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.937377</td>\n",
       "      <td>0.826369</td>\n",
       "      <td>0.791783</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.961314</td>\n",
       "      <td>0.884546</td>\n",
       "      <td>0.827399</td>\n",
       "      <td>0.846564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.977005</td>\n",
       "      <td>0.928143</td>\n",
       "      <td>0.853319</td>\n",
       "      <td>0.874254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.963088</td>\n",
       "      <td>0.873027</td>\n",
       "      <td>0.891845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991416</td>\n",
       "      <td>0.890445</td>\n",
       "      <td>0.906335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904175</td>\n",
       "      <td>0.918492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.917839</td>\n",
       "      <td>0.929491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.928717</td>\n",
       "      <td>0.938784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938114</td>\n",
       "      <td>0.947473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.947484</td>\n",
       "      <td>0.954630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955190</td>\n",
       "      <td>0.961204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961805</td>\n",
       "      <td>0.966880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968125</td>\n",
       "      <td>0.971824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973685</td>\n",
       "      <td>0.976385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>0.980380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982755</td>\n",
       "      <td>0.983882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.986646</td>\n",
       "      <td>0.987284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990404</td>\n",
       "      <td>0.990375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993409</td>\n",
       "      <td>0.993301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996274</td>\n",
       "      <td>0.995905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998490</td>\n",
       "      <td>0.998294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3\n",
       "0   0.768218  0.486046  0.485992  0.531203\n",
       "1   0.867152  0.629811  0.643634  0.671907\n",
       "2   0.908902  0.762647  0.740847  0.752590\n",
       "3   0.937377  0.826369  0.791783  0.815600\n",
       "4   0.961314  0.884546  0.827399  0.846564\n",
       "5   0.977005  0.928143  0.853319  0.874254\n",
       "6   0.989584  0.963088  0.873027  0.891845\n",
       "7   1.000000  0.991416  0.890445  0.906335\n",
       "8        NaN  1.000000  0.904175  0.918492\n",
       "9        NaN       NaN  0.917839  0.929491\n",
       "10       NaN       NaN  0.928717  0.938784\n",
       "11       NaN       NaN  0.938114  0.947473\n",
       "12       NaN       NaN  0.947484  0.954630\n",
       "13       NaN       NaN  0.955190  0.961204\n",
       "14       NaN       NaN  0.961805  0.966880\n",
       "15       NaN       NaN  0.968125  0.971824\n",
       "16       NaN       NaN  0.973685  0.976385\n",
       "17       NaN       NaN  0.978541  0.980380\n",
       "18       NaN       NaN  0.982755  0.983882\n",
       "19       NaN       NaN  0.986646  0.987284\n",
       "20       NaN       NaN  0.990404  0.990375\n",
       "21       NaN       NaN  0.993409  0.993301\n",
       "22       NaN       NaN  0.996274  0.995905\n",
       "23       NaN       NaN  0.998490  0.998294\n",
       "24       NaN       NaN  1.000000  1.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_norms_4d(x_viz_train[50].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average tensor approx pct error is 16.294653242929595\n",
      "average tensor approx pct error is 15.749946799845878\n"
     ]
    }
   ],
   "source": [
    "#function intaking x_stat, x_viz and concat them according to ranks \n",
    "def concat_viz_tensor(x_stat_train, x_viz_train, reduced_ranks): #third arg is vision tensor's dimensions \n",
    "    #get tabular data \n",
    "    X_train = x_stat_train.reshape(x_stat_train.shape[0], -1)\n",
    "\n",
    "    #get vision data according to tensor_ranks \n",
    "    viz = np.zeros((x_viz_train.shape[0], np.prod(reduced_ranks)))\n",
    "    avg_error = 0 \n",
    "    for i in range(x_viz_train.shape[0]):\n",
    "        viz[i,:], error = viz_to_arr(x_viz_train[i].numpy(), reduced_ranks)\n",
    "        avg_error += error \n",
    "    print('average tensor approx pct error is', avg_error/x_viz_train.shape[0])   \n",
    "    X_train = np.concatenate((X_train.numpy(), viz), axis=1)\n",
    "    return X_train \n",
    "\n",
    "#concat viz with tabular \n",
    "X_train =concat_viz_tensor(x_stat_train, x_viz_train, reduced_ranks=[5,7,10,10])\n",
    "X_test =concat_viz_tensor(x_stat_test, x_viz_test, reduced_ranks=[5,7,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB score 0.5453100158982512\n",
      "Baseline score 0.45151033386327505\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=5, n_estimators=80)\n",
    "xgb.fit(X_train, tgt_intensity_cat_train)\n",
    "yhat = xgb.predict(X_test)\n",
    "print(\"XGB score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2514, 8, 9, 25, 25])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_viz_train.shape\n",
    "#samples * timesteps * number of maps * size of map * size of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2514, 8, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stat_train.shape\n",
    "#samples * number of past timesteps * number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 60, 11)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "#number of storms * number of timesteps * number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2514, 80])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = x_stat_train.reshape(x_stat_train.shape[0], -1)\n",
    "X_test = x_stat_test.reshape(x_stat_test.shape[0], -1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2514, 45000])\n",
      "torch.Size([629, 45000])\n"
     ]
    }
   ],
   "source": [
    "X_train_vision = x_viz_train.reshape(x_viz_train.shape[0], -1)\n",
    "X_test_vision = x_viz_test.reshape(x_viz_test.shape[0], -1)\n",
    "print(X_train_vision.shape)\n",
    "print(X_test_vision.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tab_vision = np.concatenate((X_train, X_train_vision), axis = 1)\n",
    "X_test_tab_vision = np.concatenate((X_test, X_test_vision), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2514])\n",
      "torch.Size([629])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_intensity_cat_train.shape)\n",
    "print(tgt_intensity_cat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB score 0.5405405405405406\n",
      "Baseline score 0.45151033386327505\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=5, n_estimators=80)\n",
    "xgb.fit(X_train, tgt_intensity_cat_train)\n",
    "yhat = xgb.predict(X_test)\n",
    "print(\"XGB score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB score 0.5405405405405406\n",
      "Baseline score 0.45151033386327505\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=5, n_estimators=80)\n",
    "xgb.fit(x_stat_train.reshape(x_stat_train.shape[0], -1), tgt_intensity_cat_train)\n",
    "yhat = xgb.predict(x_stat_test.reshape(x_stat_test.shape[0], -1))\n",
    "print(\"XGB score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-47b2e8190d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tab_vision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_intensity_cat_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tab_vision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XGB score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_intensity_cat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_intensity_cat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_intensity_cat_baseline_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    821\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    207\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1248\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=6, n_estimators=200)\n",
    "xgb.fit(X_train_tab_vision, tgt_intensity_cat_train)\n",
    "yhat = xgb.predict(X_test_tab_vision)\n",
    "print(\"XGB score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=4, n_estimators=80)\n",
    "xgb.fit(X_train_vision, tgt_intensity_cat_train)\n",
    "yhat = xgb.predict(X_test_vision)\n",
    "print(\"XGB score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF score 0.8814372999709048\n",
      "Baseline score 0.8910386965376782\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf.fit(X_train, tgt_intensity_cat_train)\n",
    "yhat = rf.predict(X_test)\n",
    "print(\"RF score\", accuracy_score(tgt_intensity_cat_test, yhat))\n",
    "print(\"Baseline score\", accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:10:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:10:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGB x score 0.34135345\n",
      "XGB y score 0.33949062\n"
     ]
    }
   ],
   "source": [
    "xgb_x = XGBRegressor(max_depth=3, n_estimators=80)\n",
    "xgb_y = XGBRegressor(max_depth=3, n_estimators=80)\n",
    "xgb_x.fit(X_train, tgt_displacement_train[:,0])\n",
    "xgb_y.fit(X_train, tgt_displacement_train[:,1])\n",
    "\n",
    "yhat_x = xgb_x.predict(X_test)\n",
    "yhat_y = xgb_y.predict(X_test)\n",
    "print(\"XGB x score\", mean_absolute_error(tgt_displacement_test[:,0], yhat_x))\n",
    "print(\"XGB y score\", mean_absolute_error(tgt_displacement_test[:,1], yhat_y))\n",
    "#be careful for interpretation because the displacement is in degree + standardized + I did not code any baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7578)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_displacement_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_displacement_train[:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(steps_in, steps_out, max_depth=5):\n",
    "    train_test_split = 0.8\n",
    "    predict_at = steps_in #steps_out\n",
    "    window_size= steps_out #how many timesteps from the past to take ie steps_in\n",
    "\n",
    "    train_tensors, test_tensors = Prepro.process(vision_data, y, train_test_split, predict_at, window_size)\n",
    "    x_viz_train, x_stat_train, tgt_intensity_cat_train, tgt_intensity_cat_baseline_train, tgt_displacement_train, tgt_intensity_train = train_tensors\n",
    "    x_viz_test, x_stat_test, tgt_intensity_cat_test, tgt_intensity_cat_baseline_test, tgt_displacement_test, tgt_intensity_test = test_tensors\n",
    "\n",
    "    #reshape and concat\n",
    "    X_train = x_stat_train.reshape(x_stat_train.shape[0], -1)\n",
    "    X_test = x_stat_test.reshape(x_stat_test.shape[0], -1)\n",
    "    X_train_vision = x_viz_train.reshape(x_viz_train.shape[0], -1)\n",
    "    X_test_vision = x_viz_test.reshape(x_viz_test.shape[0], -1)\n",
    "    X_train_tab_vision = np.concatenate((X_train, X_train_vision), axis = 1)\n",
    "    X_test_tab_vision = np.concatenate((X_test, X_test_vision), axis = 1)\n",
    "\n",
    "    #run xgb for intensity\n",
    "    xgb = XGBClassifier(max_depth, n_estimators=100)\n",
    "    xgb.fit(X_train, tgt_intensity_cat_train)\n",
    "    intensity_xgb = xgb.predict(X_test)\n",
    "\n",
    "    #run random forrest for intensity\n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    rf.fit(X_train, tgt_intensity_cat_train)\n",
    "    intensity_rf = rf.predict(X_test)\n",
    "\n",
    "    #run xbg for displacement x and y\n",
    "    xgb_x = XGBRegressor(max_depth, n_estimators=100)\n",
    "    xgb_y = XGBRegressor(max_depth, n_estimators=100)\n",
    "    xgb_x.fit(X_train, tgt_displacement_train[:,0])\n",
    "    xgb_y.fit(X_train, tgt_displacement_train[:,1])\n",
    "\n",
    "    dx_xgb= xgb_x.predict(X_test)\n",
    "    dy_xgb= xgb_y.predict(X_test)\n",
    "\n",
    "    # #calculate accuracy score for intensity\n",
    "    intensity_xgb_score = accuracy_score(tgt_intensity_cat_test, intensity_xgb).round(3)\n",
    "    intensity_rf_score = accuracy_score(tgt_intensity_cat_test, intensity_rf).round(3)\n",
    "    intensity_base_score = accuracy_score(tgt_intensity_cat_test, tgt_intensity_cat_baseline_test).round(3)\n",
    "\n",
    "    #calculate displacement mae\n",
    "    dx_xgb_mae =  mean_absolute_error(tgt_displacement_test[:,0], dx_xgb).round(3)\n",
    "    dy_xgb_mae = mean_absolute_error(tgt_displacement_test[:,1], dy_xgb).round(3)\n",
    "\n",
    "    return intensity_xgb_score, intensity_rf_score, intensity_base_score, dx_xgb_mae, dy_xgb_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data:\n",
    "#vision_data = np.load('data/vision_data_30_16_120_3years_test2.npy', allow_pickle = True)\n",
    "vision_data = np.load('data/vision_data_50_20_60_3years_v2.npy', allow_pickle = True)\n",
    "# vision_data = np.load('../../../Volumes/Samsung_T5/vision_data_50_20_90_1980_v3.npy', allow_pickle = True)\n",
    "\n",
    "#y = np.load('data/y_30_16_120_3years_test2.npy', allow_pickle = True)\n",
    "y = np.load('data/y_50_20_60_3years_v2.npy', allow_pickle = True)\n",
    "# y = np.load('../../../Volumes/Samsung_T5/y_50_20_90_1980_v3.npy', allow_pickle = True)\n",
    "\n",
    "#set up empty dataframes\n",
    "accuracy = pd.DataFrame(columns={})\n",
    "\n",
    "#prediction steps\n",
    "steps_out_list= [2] #[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "#max_depth\n",
    "max_depth_list = [4,5]\n",
    "\n",
    "#steps_in\n",
    "steps_in_list = [8]\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    for steps_in in steps_in_list:\n",
    "        for steps_out in steps_out_list:\n",
    "            #run model\n",
    "            intensity_xgb_score, intensity_rf_score, intensity_base_score, dx_xgb_mae, dy_xgb_mae = run_models(steps_in, steps_out, max_depth)\n",
    "            #record accuracy\n",
    "            accuracy = accuracy.append({'past_n_steps': str(steps_in),\n",
    "                                              'pred_n_steps': str(t),\n",
    "                                              'max_depth': str(max_depth),\n",
    "                                              'xgb_intensity_accu': intensity_xgb_score,\n",
    "                                              'rf_intensity_accu': intensity_rf_score,\n",
    "                                              'base_intensity_accu': intensity_base_score,\n",
    "                                              'dx_xgb_mae':dx_xgb_mae,\n",
    "                                              'dy_xgb_mae':dy_xgb_mae}, ignore_index=True)\n",
    "\n",
    "#output results df\n",
    "accuracy.to_csv('cluster_results/model_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
