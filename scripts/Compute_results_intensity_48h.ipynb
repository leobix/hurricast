{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import prepro, metrics, run, setup\n",
    "import src.models.factory as model_factory\n",
    "import config\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from src.utils import models\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.utils.data_processing import *\n",
    "\n",
    "window_size = 8\n",
    "predict_at = 16\n",
    "full = True\n",
    "\n",
    "tgt_intensity_cat_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                      allow_pickle=True))\n",
    "tgt_intensity_cat_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "\n",
    "tgt_intensity_train = torch.Tensor(np.load('../data/y_train_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                  allow_pickle=True))\n",
    "tgt_intensity_test = torch.Tensor(np.load('../data/y_test_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                 allow_pickle=True))\n",
    "\n",
    "tgt_intensity_cat_baseline_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',  allow_pickle = True))\n",
    "tgt_intensity_cat_baseline_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "\n",
    "def standardize(tgt_displacement_train, tgt_displacement_test):\n",
    "    mean_dx = tgt_displacement_train[:, 0].mean()\n",
    "    std_dx = tgt_displacement_train[:, 0].std()\n",
    "    tgt_displacement_train[:, 0] = (tgt_displacement_train[:, 0] - mean_dx) / std_dx\n",
    "    tgt_displacement_test[:, 0] = (tgt_displacement_test[:, 0] - mean_dx) / std_dx\n",
    "    std_dx = float(std_dx)\n",
    "    mean_dx = float(mean_dx)\n",
    "    mean_dy = tgt_displacement_train[:, 1].mean()\n",
    "    std_dy = tgt_displacement_train[:, 1].std()\n",
    "    tgt_displacement_train[:, 1] = (tgt_displacement_train[:, 1] - mean_dy) / std_dy\n",
    "    tgt_displacement_test[:, 1] = (tgt_displacement_test[:, 1] - mean_dy) / std_dy\n",
    "    std_dy = float(std_dy)\n",
    "    mean_dy = float(mean_dy)\n",
    "    return tgt_displacement_train, tgt_displacement_test, std_dx, mean_dx, std_dy, mean_dy\n",
    "\n",
    "def unstandardize(tgt_displacement_train, tgt_displacement_test, std_dx, mean_dx, std_dy, mean_dy):\n",
    "    tgt_displacement_train[:, 0] = tgt_displacement_train[:, 0] *  std_dx + mean_dx\n",
    "    tgt_displacement_test[:, 0] = tgt_displacement_test[:, 0] *  std_dx + mean_dx\n",
    "    tgt_displacement_train[:, 1] = tgt_displacement_train[:, 1] * std_dy + mean_dy\n",
    "    tgt_displacement_test[:, 1] = tgt_displacement_test[:, 1] * std_dy + mean_dy\n",
    "    return tgt_displacement_train, tgt_displacement_test\n",
    "\n",
    "\n",
    "##########\n",
    "##########\n",
    "########## PREPARING DATA FOR XGB\n",
    "\n",
    "X_train = np.load('../data/X_train_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "X_test = np.load('../data/X_test_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "\n",
    "\n",
    "names = ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND',\n",
    "         'STORM_SPEED', 'cat_cos_day', 'cat_sign_day', 'COS_STORM_DIR', 'SIN_STORM_DIR',\n",
    "         'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'cat_storm_category', 'cat_basin_AN',\n",
    "         'cat_basin_EP', 'cat_basin_NI', 'cat_basin_SA',\n",
    "         'cat_basin_SI', 'cat_basin_SP', 'cat_basin_WP', 'cat_nature_DS', 'cat_nature_ET',\n",
    "         'cat_nature_MX', 'cat_nature_NR', 'cat_nature_SS', 'cat_nature_TS',\n",
    "         'STORM_DISPLACEMENT_X', 'STORM_DISPLACEMENT_Y']\n",
    "\n",
    "names_all = names * window_size\n",
    "\n",
    "for i in range(len(names_all)):\n",
    "    names_all[i] += '_' + str(i // 30)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_train.columns = names_all\n",
    "X_test.columns = names_all\n",
    "\n",
    "cols = [c for c in X_train.columns if c.lower()[-2:] == '_0' or c.lower()[:3] != 'cat']\n",
    "\n",
    "X_train = X_train[cols]\n",
    "X_test = X_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_embed = np.load('../data/embeddings/X_train_embeds_1980_34_20_120_results8_16_20_44_28.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embeds_1980_34_20_120_results8_16_20_44_28.npy', allow_pickle = True)\n",
    "#X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_intensity.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_intensity.npy', allow_pickle = True)\n",
    "\n",
    "#48\n",
    "#X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_total = np.concatenate((X_train, X_train_embed), axis = 1)\n",
    "X_test_total = np.concatenate((X_test, X_test_embed), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_24_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "\n",
    "\n",
    "#names_baselines = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'GFDL_24_lat', 'GFDL_24_lon', 'GFDL_24_vmax', 'GFDL_24_mslp', 'GFDL_24_COS_LAT', 'GFDL_24_SIN_LAT', 'GFDL_24_COS_LON', 'GFDL_24_SIN_LON', 'CMC_24_lat', 'CMC_24_lon', 'CMC_24_vmax', 'CMC_24_mslp', 'CMC_24_COS_LAT', 'CMC_24_SIN_LAT', 'CMC_24_COS_LON', 'CMC_24_SIN_LON', 'FSSE_24_lat', 'FSSE_24_lon', 'FSSE_24_vmax', 'FSSE_24_mslp', 'FSSE_24_COS_LAT', 'FSSE_24_SIN_LAT', 'FSSE_24_COS_LON', 'FSSE_24_SIN_LON', 'OFCL_24_lat', 'OFCL_24_lon', 'OFCL_24_vmax', 'OFCL_24_mslp', 'OFCL_24_COS_LAT', 'OFCL_24_SIN_LAT', 'OFCL_24_COS_LON', 'OFCL_24_SIN_LON', 'NGPS_24_lat', 'NGPS_24_lon', 'NGPS_24_vmax', 'NGPS_24_mslp', 'NGPS_24_COS_LAT', 'NGPS_24_SIN_LAT', 'NGPS_24_COS_LON', 'NGPS_24_SIN_LON', 'DSHP_24_lat', 'DSHP_24_lon', 'DSHP_24_vmax', 'DSHP_24_mslp', 'DSHP_24_COS_LAT', 'DSHP_24_SIN_LAT', 'DSHP_24_COS_LON', 'DSHP_24_SIN_LON', 'SHIP_24_lat', 'SHIP_24_lon', 'SHIP_24_vmax', 'SHIP_24_mslp', 'SHIP_24_COS_LAT', 'SHIP_24_SIN_LAT', 'SHIP_24_COS_LON', 'SHIP_24_SIN_LON', 'CLP5_24_lat', 'CLP5_24_lon', 'CLP5_24_vmax', 'CLP5_24_mslp', 'CLP5_24_COS_LAT', 'CLP5_24_SIN_LAT', 'CLP5_24_COS_LON', 'CLP5_24_SIN_LON', 'HWRF_24_lat', 'HWRF_24_lon', 'HWRF_24_vmax', 'HWRF_24_mslp', 'HWRF_24_COS_LAT', 'HWRF_24_SIN_LAT', 'HWRF_24_COS_LON', 'HWRF_24_SIN_LON', 'UKXI_24_lat', 'UKXI_24_lon', 'UKXI_24_vmax', 'UKXI_24_mslp', 'UKXI_24_COS_LAT', 'UKXI_24_SIN_LAT', 'UKXI_24_COS_LON', 'UKXI_24_SIN_LON', 'LBAR_24_lat', 'LBAR_24_lon', 'LBAR_24_vmax', 'LBAR_24_mslp', 'LBAR_24_COS_LAT', 'LBAR_24_SIN_LAT', 'LBAR_24_COS_LON', 'LBAR_24_SIN_LON', 'AEMN_24_lat', 'AEMN_24_lon', 'AEMN_24_vmax', 'AEMN_24_mslp', 'AEMN_24_COS_LAT', 'AEMN_24_SIN_LAT', 'AEMN_24_COS_LON', 'AEMN_24_SIN_LON', 'DISPLACEMENT_LAT_CLP5_24', 'DISPLACEMENT_LON_CLP5_24', 'DISPLACEMENT_LAT_SHIP_24', 'DISPLACEMENT_LON_SHIP_24', 'DISPLACEMENT_LAT_DSHP_24', 'DISPLACEMENT_LON_DSHP_24', 'DISPLACEMENT_LAT_LBAR_24', 'DISPLACEMENT_LON_LBAR_24', 'DISPLACEMENT_LAT_CMC_24', 'DISPLACEMENT_LON_CMC_24', 'DISPLACEMENT_LAT_NGPS_24', 'DISPLACEMENT_LON_NGPS_24', 'DISPLACEMENT_LAT_GFDL_24', 'DISPLACEMENT_LON_GFDL_24', 'DISPLACEMENT_LAT_HWRF_24', 'DISPLACEMENT_LON_HWRF_24', 'DISPLACEMENT_LAT_UKXI_24', 'DISPLACEMENT_LON_UKXI_24', 'DISPLACEMENT_LAT_FSSE_24', 'DISPLACEMENT_LON_FSSE_24', 'DISPLACEMENT_LAT_AEMN_24', 'DISPLACEMENT_LON_AEMN_24', 'DISPLACEMENT_LAT_OFCL_24', 'DISPLACEMENT_LON_OFCL_24', 'EMXI_24_lat', 'EMXI_24_lon', 'EMXI_24_vmax', 'EMXI_24_mslp', 'EMXI_24_COS_LAT', 'EMXI_24_SIN_LAT', 'EMXI_24_COS_LON', 'EMXI_24_SIN_LON', 'DISPLACEMENT_LAT_EMXI_24', 'DISPLACEMENT_LON_EMXI_24', 'GFSO_24_lat', 'GFSO_24_lon', 'GFSO_24_vmax', 'GFSO_24_mslp', 'GFSO_24_COS_LAT', 'GFSO_24_SIN_LAT', 'GFSO_24_COS_LON', 'GFSO_24_SIN_LON', 'DISPLACEMENT_LAT_GFSO_24', 'DISPLACEMENT_LON_GFSO_24', 'cat_basin_AN', 'cat_basin_EP', 'basin_NI', 'basin_SI', 'basin_SP', 'basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n",
    "names_baselines = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'DSHP_24_lat', 'DSHP_24_lon', 'DSHP_24_vmax', 'DSHP_24_mslp', 'DSHP_24_COS_LAT', 'DSHP_24_SIN_LAT', 'DSHP_24_COS_LON', 'DSHP_24_SIN_LON', 'OFCL_24_lat', 'OFCL_24_lon', 'OFCL_24_vmax', 'OFCL_24_mslp', 'OFCL_24_COS_LAT', 'OFCL_24_SIN_LAT', 'OFCL_24_COS_LON', 'OFCL_24_SIN_LON', 'UKXI_24_lat', 'UKXI_24_lon', 'UKXI_24_vmax', 'UKXI_24_mslp', 'UKXI_24_COS_LAT', 'UKXI_24_SIN_LAT', 'UKXI_24_COS_LON', 'UKXI_24_SIN_LON', 'CMC_24_lat', 'CMC_24_lon', 'CMC_24_vmax', 'CMC_24_mslp', 'CMC_24_COS_LAT', 'CMC_24_SIN_LAT', 'CMC_24_COS_LON', 'CMC_24_SIN_LON', 'SHIP_24_lat', 'SHIP_24_lon', 'SHIP_24_vmax', 'SHIP_24_mslp', 'SHIP_24_COS_LAT', 'SHIP_24_SIN_LAT', 'SHIP_24_COS_LON', 'SHIP_24_SIN_LON', 'FSSE_24_lat', 'FSSE_24_lon', 'FSSE_24_vmax', 'FSSE_24_mslp', 'FSSE_24_COS_LAT', 'FSSE_24_SIN_LAT', 'FSSE_24_COS_LON', 'FSSE_24_SIN_LON', 'CLP5_24_lat', 'CLP5_24_lon', 'CLP5_24_vmax', 'CLP5_24_mslp', 'CLP5_24_COS_LAT', 'CLP5_24_SIN_LAT', 'CLP5_24_COS_LON', 'CLP5_24_SIN_LON', 'AEMN_24_lat', 'AEMN_24_lon', 'AEMN_24_vmax', 'AEMN_24_mslp', 'AEMN_24_COS_LAT', 'AEMN_24_SIN_LAT', 'AEMN_24_COS_LON', 'AEMN_24_SIN_LON', 'LBAR_24_lat', 'LBAR_24_lon', 'LBAR_24_vmax', 'LBAR_24_mslp', 'LBAR_24_COS_LAT', 'LBAR_24_SIN_LAT', 'LBAR_24_COS_LON', 'LBAR_24_SIN_LON', 'GFDL_24_lat', 'GFDL_24_lon', 'GFDL_24_vmax', 'GFDL_24_mslp', 'GFDL_24_COS_LAT', 'GFDL_24_SIN_LAT', 'GFDL_24_COS_LON', 'GFDL_24_SIN_LON', 'HWRF_24_lat', 'HWRF_24_lon', 'HWRF_24_vmax', 'HWRF_24_mslp', 'HWRF_24_COS_LAT', 'HWRF_24_SIN_LAT', 'HWRF_24_COS_LON', 'HWRF_24_SIN_LON', 'NGPS_24_lat', 'NGPS_24_lon', 'NGPS_24_vmax', 'NGPS_24_mslp', 'NGPS_24_COS_LAT', 'NGPS_24_SIN_LAT', 'NGPS_24_COS_LON', 'NGPS_24_SIN_LON', 'DISPLACEMENT_LAT_CLP5_24', 'DISPLACEMENT_LON_CLP5_24', 'DISPLACEMENT_LAT_SHIP_24', 'DISPLACEMENT_LON_SHIP_24', 'DISPLACEMENT_LAT_DSHP_24', 'DISPLACEMENT_LON_DSHP_24', 'DISPLACEMENT_LAT_LBAR_24', 'DISPLACEMENT_LON_LBAR_24', 'DISPLACEMENT_LAT_CMC_24', 'DISPLACEMENT_LON_CMC_24', 'DISPLACEMENT_LAT_NGPS_24', 'DISPLACEMENT_LON_NGPS_24', 'DISPLACEMENT_LAT_GFDL_24', 'DISPLACEMENT_LON_GFDL_24', 'DISPLACEMENT_LAT_HWRF_24', 'DISPLACEMENT_LON_HWRF_24', 'DISPLACEMENT_LAT_UKXI_24', 'DISPLACEMENT_LON_UKXI_24', 'DISPLACEMENT_LAT_FSSE_24', 'DISPLACEMENT_LON_FSSE_24', 'DISPLACEMENT_LAT_AEMN_24', 'DISPLACEMENT_LON_AEMN_24', 'DISPLACEMENT_LAT_OFCL_24', 'DISPLACEMENT_LON_OFCL_24', 'EMXI_24_lat', 'EMXI_24_lon', 'EMXI_24_vmax', 'EMXI_24_mslp', 'EMXI_24_COS_LAT', 'EMXI_24_SIN_LAT', 'EMXI_24_COS_LON', 'EMXI_24_SIN_LON', 'DISPLACEMENT_LAT_EMXI_24', 'DISPLACEMENT_LON_EMXI_24', 'GFSO_24_lat', 'GFSO_24_lon', 'GFSO_24_vmax', 'GFSO_24_mslp', 'GFSO_24_COS_LAT', 'GFSO_24_SIN_LAT', 'GFSO_24_COS_LON', 'GFSO_24_SIN_LON', 'DISPLACEMENT_LAT_GFSO_24', 'DISPLACEMENT_LON_GFSO_24', 'cat_basin_AN', 'cat_basin_EP', 'basin_NI', 'basin_SI', 'basin_SP', 'basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n",
    "names_all_baselines = names_baselines * 8#args.window_size\n",
    "\n",
    "for i in range(len(names_all_baselines)):\n",
    "    names_all_baselines[i] += '_' + str(i // 167)\n",
    "\n",
    "X_test_baseline.columns = names_all_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_test_baseline.shape[0]\n",
    "X_test_total = X_test_total[-n:]\n",
    "tgt_intensity_test = tgt_intensity_test[-n:]\n",
    "X_test = X_test[-n:]\n",
    "X_test_embed = X_test_embed[-n:]\n",
    "m = len(X_test_baseline[X_test_baseline['YEAR_0'] < 2017])\n",
    "\n",
    "if full:\n",
    "    tgt_intensity_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:m]), axis = 0)\n",
    "    X_train = pd.concat((X_train, X_test[:m]), axis = 0)\n",
    "    X_train_embed = np.concatenate((X_train_embed, X_test_embed[:m]), axis = 0)\n",
    "    X_train_total = np.concatenate((X_train_total, X_test_total[:m]), axis = 0)\n",
    "    \n",
    "mean_intensity = tgt_intensity_train.mean()\n",
    "std_intensity = tgt_intensity_train.std()\n",
    "tgt_intensity_train = (tgt_intensity_train - mean_intensity)/std_intensity\n",
    "tgt_intensity_test = (tgt_intensity_test - mean_intensity)/std_intensity\n",
    "\n",
    "std_ = float(std_intensity)\n",
    "mean_ = float(mean_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = X_train[np.round(X_train['WMO_WIND_7']*1000%10, decimals = 2) == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[np.round(X_train['WMO_WIND_7']*1000%10, decimals = 2) == 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_total = X_train_total[index]\n",
    "tgt_intensity_train = tgt_intensity_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = X_train_embed[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  12.559202\n",
      "MAE intensity:  11.910126\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBRegressor(max_depth=6, n_estimators=140, learning_rate = 0.07, subsample = 0.7, min_child_weight = 5)\n",
    "xgb2.fit(X_train, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb2.predict(X_test))*std_+mean_))\n",
    "\n",
    "xgb = XGBRegressor(max_depth=8, n_estimators = 150, learning_rate = 0.07, subsample = 0.7, min_child_weight = 1)\n",
    "xgb.fit(X_train_total, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb.predict(X_test_total))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_track(basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    train_x = X_train_total\n",
    "    train_y = X_train_total\n",
    "    test_x = X_test_total\n",
    "    test_y = X_test_total\n",
    "    tgt_train = tgt_displacement_train\n",
    "    if sparse:\n",
    "        train_x, train_y = X_train_total_sparse_x, X_train_total_sparse_y\n",
    "        test_x, test_y = X_test_total_sparse_x, X_test_total_sparse_y\n",
    "    if basin_only:\n",
    "        train_x = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        train_y = train_x\n",
    "        tgt_train = tgt_displacement_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "    xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "    DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "    DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "    LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "    LONS_PRED_ = X_test['LON_7'] + DLONS_PRED\n",
    "    compare_perf_track(basin=basin, forecast=forecast, forecast2 = forecast2, LATS_PRED_=LATS_PRED_, LONS_PRED_=LONS_PRED_)\n",
    "\n",
    "def train_xgb_track_all_years(use_forecast = False, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    train_x = X_train_total\n",
    "    train_y = X_train_total\n",
    "    test_x = X_test_total\n",
    "    test_y = X_test_total\n",
    "    tgt_train = tgt_displacement_train\n",
    "    if sparse:\n",
    "        train_x, train_y = X_train_total_sparse_x, X_train_total_sparse_y\n",
    "        test_x, test_y = X_test_total_sparse_x, X_test_total_sparse_y\n",
    "    if basin_only:\n",
    "        train_x = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        train_y = train_x\n",
    "        tgt_train = tgt_displacement_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    if use_forecast:\n",
    "        train_for = X_train_forecasts\n",
    "        tgt_train_for = tgt_train_dis_forecasts\n",
    "        test_for = X_test_forecasts\n",
    "        xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_x.fit(train_for, tgt_train_for[:, 0])\n",
    "        xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_y.fit(train_for, tgt_train_for[:, 1])\n",
    "        DLATS_PRED = np.array(xgb_x.predict(X_new)) * std_dx + mean_dx\n",
    "        DLONS_PRED = np.array(xgb_y.predict(X_new)) * std_dy + mean_dy\n",
    "    else:\n",
    "        xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "        xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "        DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "        DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "    LATS_PRED_2012 = X_test['LAT_7'] + DLATS_PRED\n",
    "    LONS_PRED_2012 = X_test['LON_7'] + DLONS_PRED\n",
    "    compare_perf_track(basin=basin, forecast=forecast, forecast2 = forecast2, LATS_PRED_=LATS_PRED_2012, LONS_PRED_=LONS_PRED_2012)\n",
    "    dict = {'year': [], 'num_samples': [], 'MAEs_full': [], 'std_full': [], 'MAES_2012': [], 'std_2012': [],\n",
    "            'MAES_SHIP': [], 'std_SHIP': [], 'MAES_HWRF': [], 'std_HWRF': []}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[\n",
    "                X_test_baseline['YEAR'] < year].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis=0)\n",
    "            tgt_train = np.concatenate((tgt_displacement_train, tgt_displacement_test[index]), axis=0)\n",
    "            xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                 subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_x.fit(train, tgt_train[:, 0])\n",
    "            xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                 subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_y.fit(train, tgt_train[:, 1])\n",
    "            DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "            DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "            LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "            LONS_PRED_ = X_test['LON_7'] + DLONS_PRED\n",
    "            compare_perf_track_per_year(dict, LATS_PRED_, LONS_PRED_, LATS_PRED_2012, LONS_PRED_2012, forecast=forecast,\n",
    "                                            forecast2=forecast2, basin=basin,  year=year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_xgb_intensity(forecast = 'SHIP', basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast2 = None):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total.fit(train, tgt_train)\n",
    "    compare_perf_intensity(xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', forecast2 = forecast2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_perf_intensity(xgb_total, basin = 'AN', forecast = 'SHIP', last_storms = 1000, mode = 'vmax', forecast2 = 'HWRF'):\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    else:\n",
    "        index = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    X_test_withBASELINE_total = X_test_total[index]\n",
    "    baseline_1 = baseline_[forecast + '_24_' + mode + '_7']\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Timesteps\", len(tgt_))\n",
    "        print(\"MAE intensity basin \" + basin + \" Hurricast : \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds), decimals=2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \" + str(forecast2) + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_2), decimals=2), \"with std \",\n",
    "              np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "        print(\"Percentage of missed intensification > 20kn Hurricast: \", np.around(sum(abs(tgt_ - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(\"Percentage of missed intensification > 20kn Official Forecast\"+ forecast + \" : \", np.around(sum(abs(tgt_ - baseline_1) > 20) / len(baseline_1) * 100, decimals =2))\n",
    "        print(\"Percentage of missed intensification > 20kn Official Forecast 2\"+ str(forecast2) + \" : \", np.around(sum(abs(tgt_ - baseline_2) > 20) / len(baseline_2) * 100, decimals =2))\n",
    "        print(\"\\nMAE intensity basin \" + basin + \" Hurricast last\", last_storms, \": \", np.around(mean_absolute_error(tgt_[-last_storms:], preds[-last_storms:]), decimals=2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \" + forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_[-last_storms:], baseline_1[-last_storms:]), decimals=2))\n",
    "\n",
    "\n",
    "#train_xgb_intensity(forecast = 'SHIP', basin = 'EP', max_depth=8, n_estimators = 120, learning_rate = 0.07, subsample = 0.8, min_child_weight = 1)\n",
    "#train_xgb_intensity(forecast = 'SHIP', basin = 'AN', max_depth=8, n_estimators = 150, learning_rate = 0.07, subsample = 0.8, min_child_weight = 1, forecast2 = 'HWRF')\n",
    "\n",
    "\n",
    "def compare_perf_intensity_per_year(dict, xgb_tot, xgb_total, year, forecast2, basin = 'AN', forecast = 'HWRF', mode = 'vmax'):\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    else:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1].index#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        #X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    X_test_withBASELINE_total = X_test_total[index]\n",
    "    baseline_1 = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        print(\"Total number of steps for comparison: \", len(tgt_))\n",
    "        preds_1 = xgb_tot.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained full: \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained until 2012: \", np.around(mean_absolute_error(tgt_, preds_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "        if forecast2 != None:\n",
    "            print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \" + forecast2 + \" : \",\n",
    "                np.around(mean_absolute_error(tgt_, baseline_2), decimals=2), \"with std \",\n",
    "                np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "        append_dict_intensity(dict, tgt_, preds, preds_1, baseline_1, baseline_2, year)\n",
    "        #print(\"Year \", year, \" Percentage of missed intensification > 20kn Hurricast: \", np.around(sum(tgt_ - preds > 20)/len(preds) * 100, decimals = 2))\n",
    "        #print(\"Year \", year, \" Percentage of missed intensification > 20kn Official Forecast: \", np.around(sum(tgt_ - baseline_1 > 20) / len(baseline_1) * 100, decimals =2))\n",
    "\n",
    "\n",
    "\n",
    "def append_dict_intensity(dict, tgt_, preds, preds_1, baseline_1, baseline_2, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(tgt_))\n",
    "    dict['MAEs_full'].append(np.around(mean_absolute_error(tgt_, preds), decimals = 2))\n",
    "    dict['std_full'].append(np.around(np.std(tgt_ - preds), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(mean_absolute_error(tgt_, preds_1), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(mean_absolute_error(tgt_, baseline_2), decimals=2))\n",
    "    dict['std_HWRF'].append(np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "\n",
    "\n",
    "def train_xgb_intensity_all_years(forecast2 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        #test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total.fit(train, tgt_train)\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            compare_perf_intensity_per_year(forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "\n",
    "def train_xgb_intensity_all_years_full_train(forecast2 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        #test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[]}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year(dict = dict, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.07,\n",
       "  'min_child_weight': 5,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.7},\n",
       " -0.2854653795560201)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'min_child_weight':[1,5],\n",
    " 'n_estimators':[100, 120, 150],\n",
    " 'subsample':[0.6,0.7,0.8,],\n",
    " 'learning_rate':[0.07, 0.1, 0.15],\n",
    "}\n",
    "grid = GridSearchCV(estimator = XGBRegressor(learning_rate = 0.07, n_estimators=140, max_depth=8,\n",
    "min_child_weight=1, subsample=0.8, seed=1),\n",
    "param_grid = params, n_jobs=4, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "grid.fit(X_train_total, np.array(tgt_intensity_train))\n",
    "\n",
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_intensity_all_years_full_train_4cast(forecast2 = None, forecast3 = None, forecast4 = None, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_FSSE':[], 'std_FSSE':[], 'MAES_OFCL':[], 'std_OFCL':[]}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year_4cast(test, dict = dict, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year, forecast3= forecast3, forecast4 = forecast4)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n",
    "\n",
    "def train_xgb_intensity_all_years_full_train_6cast(forecast2 = None, forecast3 = None, forecast4 = None, forecast5 = None, forecast6 = None, max_depth = 8, n_estimators = 140, learning_rate = 0.07, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'SHIP'):\n",
    "    train = X_train_total\n",
    "    test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict6 = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_FSSE':[], 'std_FSSE':[], 'MAES_OFCL':[], 'std_OFCL':[], 'MAES_GFSO':[], 'std_GFSO':[], 'MAES_DSHP':[], 'std_DSHP':[]}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year_6cast(test, dict6, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, year = year, forecast3= forecast3, forecast4 = forecast4, forecast5 = forecast5, forecast6 = forecast6)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict6\n",
    "\n",
    "def compare_perf_intensity_per_year_4cast(test, dict, xgb_tot, xgb_total, year, forecast2, forecast3, forecast4, basin = 'AN', forecast = 'SHIP', mode = 'vmax'):\n",
    "    baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "    if basin == 'EP':\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] < 2019].loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]\n",
    "    index = baseline_.index\n",
    "    baseline_1 = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    baseline_3 = baseline_[forecast3 + '_24_' + mode + '_7']\n",
    "    baseline_4 = baseline_[forecast4 + '_24_' + mode + '_7']\n",
    "    X_test_withBASELINE_total = np.array(test)[index]\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        print(\"Total number of steps for comparison: \", len(tgt_))\n",
    "        print(\"Year\", year, \"Basin\", basin, \"MAE intensity | std | busts > 20kn %\")\n",
    "        preds_1 = xgb_tot.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Hurr full: \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), np.around(np.std(tgt_ - preds), decimals=2), np.around(sum(abs(tgt_ - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(\"Hurr until 2012: \", np.around(mean_absolute_error(tgt_, preds_1), decimals = 2), np.around(np.std(tgt_ - preds_1), decimals=2), np.around(sum(abs(tgt_ - preds_1) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(forecast,\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), np.around(np.std(tgt_ - baseline_1), decimals = 2), np.around(sum(abs(tgt_ - baseline_1) > 20)/len(preds) * 100, decimals = 2))\n",
    "        if forecast2 != None:\n",
    "            print(forecast2,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_2), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_2), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_2) > 20)/len(preds) * 100, decimals = 2))\n",
    "            print(forecast3,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_3), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_3), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_3) > 20)/len(preds) * 100, decimals = 2))\n",
    "            print(forecast4,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_4), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_4), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_4) > 20)/len(preds) * 100, decimals = 2))\n",
    "\n",
    "        append_dict_intensity_4cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, year)\n",
    "        \n",
    "        \n",
    "def append_dict_intensity_4cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(tgt_))\n",
    "    dict['MAEs_full'].append(np.around(mean_absolute_error(tgt_, preds), decimals = 2))\n",
    "    dict['std_full'].append(np.around(np.std(tgt_ - preds), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(mean_absolute_error(tgt_, preds_1), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(mean_absolute_error(tgt_, baseline_2), decimals=2))\n",
    "    dict['std_HWRF'].append(np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "    dict['MAES_OFCL'].append(np.around(mean_absolute_error(tgt_, baseline_3), decimals=2))\n",
    "    dict['std_OFCL'].append(np.around(np.std(tgt_ - baseline_3), decimals=2))\n",
    "    dict['MAES_FSSE'].append(np.around(mean_absolute_error(tgt_, baseline_4), decimals=2))\n",
    "    dict['std_FSSE'].append(np.around(np.std(tgt_ - baseline_4), decimals=2))\n",
    "    \n",
    "def append_dict_intensity_6cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(tgt_))\n",
    "    dict['MAEs_full'].append(np.around(mean_absolute_error(tgt_, preds), decimals = 2))\n",
    "    dict['std_full'].append(np.around(np.std(tgt_ - preds), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(mean_absolute_error(tgt_, preds_1), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(mean_absolute_error(tgt_, baseline_2), decimals=2))\n",
    "    dict['std_HWRF'].append(np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "    dict['MAES_OFCL'].append(np.around(mean_absolute_error(tgt_, baseline_3), decimals=2))\n",
    "    dict['std_OFCL'].append(np.around(np.std(tgt_ - baseline_3), decimals=2))\n",
    "    dict['MAES_FSSE'].append(np.around(mean_absolute_error(tgt_, baseline_4), decimals=2))\n",
    "    dict['std_FSSE'].append(np.around(np.std(tgt_ - baseline_4), decimals=2))\n",
    "    dict['MAES_GFSO'].append(np.around(mean_absolute_error(tgt_, baseline_5), decimals=2))\n",
    "    dict['std_GFSO'].append(np.around(np.std(tgt_ - baseline_5), decimals=2))\n",
    "    dict['MAES_DSHP'].append(np.around(mean_absolute_error(tgt_, baseline_6), decimals=2))\n",
    "    dict['std_DSHP'].append(np.around(np.std(tgt_ - baseline_6), decimals=2))\n",
    "    \n",
    "    #consensus_ofcl = (baseline_1+baseline_2+baseline_3+baseline_4+baseline_5+baseline_6)/6\n",
    "    #consensus_hurr = (baseline_1+baseline_2+baseline_3+baseline_4+baseline_5+baseline_6+preds)/7\n",
    "    consensus_ofcl = (baseline_1+baseline_2+baseline_3+baseline_4 + baseline_6)/5\n",
    "    consensus_hurr = (baseline_1+baseline_2+baseline_3+baseline_4 + baseline_6+preds)/6\n",
    "    print(\"Consensus ofcl\", np.around(mean_absolute_error(tgt_, consensus_ofcl), decimals=2), np.around(np.std(tgt_ - consensus_ofcl), decimals = 2), np.around(sum(abs(tgt_ - consensus_ofcl) > 20)/len(preds) * 100, decimals = 2))\n",
    "    print(\"Consensus Hurr\", np.around(mean_absolute_error(tgt_, consensus_hurr), decimals=2), np.around(np.std(tgt_ - consensus_hurr), decimals = 2), np.around(sum(abs(tgt_ - consensus_hurr) > 20)/len(preds) * 100, decimals = 2))\n",
    "    \n",
    "    \n",
    "def compare_perf_intensity_per_year_6cast(test, dict, xgb_tot, xgb_total, year, forecast2, forecast3, forecast4, forecast5, forecast6, basin = 'AN', forecast = 'SHIP'):\n",
    "    mode = 'vmax'\n",
    "    baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "    if basin == 'EP':\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] < 2019].loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]\n",
    "    index = baseline_.index\n",
    "    baseline_1 = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    baseline_3 = baseline_[forecast3 + '_24_' + mode + '_7']\n",
    "    baseline_4 = baseline_[forecast4 + '_24_' + mode + '_7']\n",
    "    baseline_5 = baseline_[forecast5 + '_24_' + mode + '_7']\n",
    "    baseline_6 = baseline_[forecast6 + '_24_' + mode + '_7']\n",
    "    X_test_withBASELINE_total = np.array(test)[index]\n",
    "    \n",
    "    tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "    print(\"Total number of steps for comparison: \", len(tgt_))\n",
    "    print(\"Year\", year, \"Basin\", basin, \"MAE intensity | std | busts > 20kn %\")\n",
    "    preds_1 = xgb_tot.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "    preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "    print(\"Hurr full: \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), np.around(np.std(tgt_ - preds), decimals=2), np.around(sum(abs(tgt_ - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "    print(\"Hurr until 2012: \", np.around(mean_absolute_error(tgt_, preds_1), decimals = 2), np.around(np.std(tgt_ - preds_1), decimals=2), np.around(sum(abs(tgt_ - preds_1) > 20)/len(preds) * 100, decimals = 2))\n",
    "    print(forecast,\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), np.around(np.std(tgt_ - baseline_1), decimals = 2), np.around(sum(abs(tgt_ - baseline_1) > 20)/len(preds) * 100, decimals = 2))\n",
    "    if forecast2 != None:\n",
    "        print(forecast2,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_2), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_2), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_2) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(forecast3,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_3), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_3), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_3) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(forecast4,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_4), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_4), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_4) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(forecast5,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_5), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_5), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_5) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(forecast6,\n",
    "                np.around(mean_absolute_error(tgt_, baseline_6), decimals=2),\n",
    "                np.around(np.std(tgt_ - baseline_6), decimals=2),\n",
    "                np.around(sum(abs(tgt_ - baseline_6) > 20)/len(preds) * 100, decimals = 2))\n",
    "\n",
    "    append_dict_intensity_6cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6, year)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#48 EP\n",
    "{'MAES_2012': [16.65, 17.68, 16.56, 11.43, 19.61, 4.58],\n",
    " 'MAES_FSSE': [13.57, 11.64, 11.12, 10.34, 12.83, 20.76],\n",
    " 'MAES_HWRF': [15.53, 14.44, 12.54, 11.16, 17.52, 23.69],\n",
    " 'MAES_OFCL': [13.61, 11.26, 11.24, 9.75, 12.56, 21.23],\n",
    " 'MAES_SHIP': [13.46, 12.75, 11.17, 8.35, 12.59, 16.01],\n",
    " 'MAEs_full': [16.16, 15.76, 16.23, 10.34, 19.29, 5.3],\n",
    " 'num_samples': [205, 221, 259, 140, 315, 37],\n",
    " 'std_2012': [20.16, 19.65, 21.14, 13.71, 23.47, 5.77],\n",
    " 'std_FSSE': [15.51, 15.72, 14.16, 12.57, 15.3, 19.16],\n",
    " 'std_HWRF': [19.13, 18.8, 15.9, 13.02, 20.54, 19.81],\n",
    " 'std_OFCL': [16.65, 16.08, 14.69, 11.23, 16.02, 19.8],\n",
    " 'std_SHIP': [16.03, 15.94, 15.2, 10.66, 16.6, 17.82],\n",
    " 'std_full': [19.51, 17.97, 20.97, 12.3, 23.67, 6.5],\n",
    " 'year': [2014, 2015, 2016, 2017, 2018, 2019]}\n",
    "\n",
    "#48 AN\n",
    "{'MAES_2012': [11.17, 23.33, 12.2, 16.07, 16.78, 17.89, 17.38, 16.59],\n",
    " 'MAES_FSSE': [10.53, 16.12, 10.41, 9.23, 10.76, 9.37, 12.62, 10.07],\n",
    " 'MAES_HWRF': [10.39, 37.12, 10.97, 11.8, 12.48, 16.58, 16.25, 15.48],\n",
    " 'MAES_OFCL': [7.23, 15.62, 7.62, 9.06, 9.77, 11.26, 11.65, 10.31],\n",
    " 'MAES_SHIP': [8.2, 14.62, 12.7, 10.04, 11.38, 11.85, 11.81, 12.3],\n",
    " 'MAEs_full': [11.05, 19.54, 12.24, 15.34, 17.2, 18.07, 17.68, 16.88],\n",
    " 'num_samples': [37, 4, 61, 101, 200, 200, 200, 179],\n",
    " 'std_2012': [13.25, 8.36, 14.72, 21.36, 20.42, 21.7, 22.88, 20.23],\n",
    " 'std_FSSE': [9.93, 9.36, 10.75, 12.95, 14.56, 12.63, 17.08, 12.78],\n",
    " 'std_HWRF': [12.46, 11.24, 12.74, 15.01, 17.19, 22.44, 20.24, 18.51],\n",
    " 'std_OFCL': [9.64, 2.07, 9.51, 12.34, 13.33, 14.9, 17.13, 13.26],\n",
    " 'std_SHIP': [9.99, 11.62, 14.58, 14.63, 16.17, 14.8, 17.3, 16.36],\n",
    " 'std_full': [12.09, 9.04, 14.51, 20.63, 20.63, 21.95, 23.03, 20.49],\n",
    " 'year': [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table intensity AN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "n = X_test_baseline.shape[0]\n",
    "X_test_total = X_test_total[-n:]\n",
    "tgt_intensity_test = tgt_intensity_test[-n:]\n",
    "X_test = X_test[-n:]\n",
    "X_test_embed = X_test_embed[-n:]\n",
    "m = len(X_test_baseline[X_test_baseline['YEAR_0'] < 2017]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict6 = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_OFCL':[], 'std_OFCL':[], 'MAES_FSSE':[], 'std_FSSE':[], 'MAES_GFSO':[], 'std_GFSO':[], 'MAES_DSHP':[], 'std_DSHP':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:m]), axis = 0)\n",
    "tgt_train5 = tgt_train*std_+mean_\n",
    "mean_intensity2 = tgt_train5.mean()\n",
    "std_intensity2 = tgt_train5.std()\n",
    "tgt_train5 = (tgt_train5 - mean_intensity2)/std_intensity2\n",
    "tgt_intensity_test = (tgt_intensity_test*std_+mean_ - mean_intensity2)/std_intensity2\n",
    "std_ = float(std_intensity2)\n",
    "mean_ = float(mean_intensity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:m]), axis = 0)\n",
    "xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "LONS_PRED_ = X_test['LON_7'] + DLONS_PRED\n",
    "compare_perf_track(basin=basin, forecast=forecast, forecast2 = forecast2, LATS_PRED_=LATS_PRED_, LONS_PRED_=LONS_PRED_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.63 22.57 37.26\n",
      "Hurr until 2012:  17.63 22.57 37.26\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.71 15.51 16.29\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  16.2 20.96 30.55\n",
      "Hurr until 2012:  16.2 20.96 30.55\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.03 15.37 19.56\n"
     ]
    }
   ],
   "source": [
    "#FULL TRAIN WITH CNN/GRU EMBEDS + ONLY AN EP in training + only more than. 40kn winds\n",
    "index_t = X_train.loc[X_train['cat_basin_AN_0'] + X_train['cat_basin_EP_0'] == 1].loc[X_train['WMO_WIND_7'] > 34].index\n",
    "index_te = X_test[:m].loc[X_test['cat_basin_AN_0'] + X_test['cat_basin_EP_0'] == 1].loc[X_test['WMO_WIND_7'] > 34].index\n",
    "train = np.concatenate((X_train_total[index_t], X_test_total[index_te]), axis = 0)\n",
    "tgt_train = np.concatenate((tgt_intensity_train[index_t], tgt_intensity_test[index_te]), axis = 0)\n",
    "\n",
    "xgb_total = XGBRegressor(max_depth=6, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=1)\n",
    "xgb_total.fit(train, tgt_train)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4104736654d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat_basin_AN_0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat_basin_EP_0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WMO_WIND_7'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindex_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat_basin_AN_0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cat_basin_EP_0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WMO_WIND_7'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_te\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtgt_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_intensity_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_intensity_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_te\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_intensity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_intensity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstd_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m         \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2405\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2407\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2408\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m   4219\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4223\u001b[0m     def drop(\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4512\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4513\u001b[0m         return self._reindex_axes(\n\u001b[0;32m-> 4514\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4515\u001b[0m         ).__finalize__(self)\n\u001b[1;32m   4516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   4533\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4534\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4535\u001b[0;31m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4536\u001b[0m             )\n\u001b[1;32m   4537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4575\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4576\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4577\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4578\u001b[0m             )\n\u001b[1;32m   4579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   3360\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3362\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "index_t = X_train.loc[X_train['cat_basin_AN_0'] + X_train['cat_basin_EP_0'] == 1].loc[X_train['WMO_WIND_7'] > 34].index\n",
    "index_te = X_test[:m].loc[X_test['cat_basin_AN_0'] + X_test['cat_basin_EP_0'] == 1].loc[X_test['WMO_WIND_7'] > 34].index\n",
    "train = np.concatenate((X_train_total[index_t], X_test_total[index_te]), axis = 0)\n",
    "tgt_train = np.array(((np.concatenate((tgt_intensity_train[index_t], tgt_intensity_test[index_te]), axis = 0))*float(std_intensity)+float(mean_intensity)-mean_)/std_)\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=1)\n",
    "xgb_total.fit(train, tgt_train)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  16.82 21.66 33.8\n",
      "Hurr until 2012:  16.82 21.66 33.8\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.62 15.4 15.42\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  15.49 20.79 30.11\n",
      "Hurr until 2012:  15.49 20.79 30.11\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.99 15.36 19.78\n"
     ]
    }
   ],
   "source": [
    "#FULL TRAIN WITH CNN/GRU EMBEDS\n",
    "index_t = X_train.loc[X_train['WMO_WIND_7'] > 25].index\n",
    "index_te = X_test[:m].loc[X_test['WMO_WIND_7'] > 25].index\n",
    "train = np.concatenate((X_train_total[index_t], X_test_total[index_te]), axis = 0)\n",
    "tgt_train = np.concatenate((tgt_intensity_train[index_t], tgt_intensity_test[index_te]), axis = 0)\n",
    "\n",
    "xgb_total = XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.05, subsample=0.8, min_child_weight=5)\n",
    "#EP 15.4\n",
    "#xgb_total = XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.05, subsample=0.8, min_child_weight=5)\n",
    "#AN 16.87: XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=5)\n",
    "\n",
    "xgb_total.fit(train, tgt_train)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.05 21.97 35.53\n",
      "Hurr until 2012:  18.85 23.93 37.26\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.67 15.43 15.6\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  15.8 21.05 30.11\n",
      "Hurr until 2012:  17.85 22.87 35.82\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.0 15.38 19.78\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(max_depth=8, n_estimators=150)\n",
    "m.fit(train, tgt_train)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = m, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = m, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-bffaa7cebad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#FULL TRAIN WITH CNN/GRU EMBEDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:17094]), axis = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "#FULL TRAIN WITH CNN/GRU EMBEDS\n",
    "train = np.concatenate((X_train_total, X_test_total[:m]), axis = 0)\n",
    "#tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:17094]), axis = 0)\n",
    "\n",
    "xgb_total = XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=1)\n",
    "xgb_total.fit(train, tgt_train5)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  669\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  10.54 13.79 13.0\n",
      "Hurr until 2012:  10.54 13.79 13.0\n",
      "SHIP 11.29 15.51 17.34\n",
      "HWRF 9.59 12.59 11.21\n",
      "OFCL 8.55 11.85 7.32\n",
      "FSSE 8.44 11.29 7.32\n",
      "GFSO 14.0 15.64 22.42\n",
      "DSHP 10.28 13.59 14.8\n",
      "Consensus ofcl 8.44 11.35 8.52\n",
      "Consensus Hurr 8.33 11.21 8.37\n",
      "Total number of steps for comparison:  522\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.27 15.24 16.28\n",
      "Hurr until 2012:  11.27 15.24 16.28\n",
      "SHIP 11.97 15.22 16.86\n",
      "HWRF 9.95 14.02 14.18\n",
      "OFCL 9.55 12.99 7.85\n",
      "FSSE 9.22 12.38 9.96\n",
      "GFSO 16.95 19.67 31.99\n",
      "DSHP 11.85 15.13 16.86\n",
      "Consensus ofcl 9.39 12.52 10.73\n",
      "Consensus Hurr 9.35 12.54 10.73\n"
     ]
    }
   ],
   "source": [
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "forecast='SHIP'\n",
    "forecast2 = 'HWRF'\n",
    "forecast3 = 'OFCL'\n",
    "forecast4 = 'FSSE'\n",
    "forecast5 = 'GFSO'\n",
    "forecast6 = 'DSHP'\n",
    "mode = 'vmax'\n",
    "basin = 'EP'\n",
    "test = X_test_total\n",
    "if True:\n",
    "    baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "    if basin == 'EP':\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]\n",
    "    index = baseline_.index\n",
    "    baseline_1 = baseline_[forecast + '_24_'+ mode + '_7']\n",
    "    baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    baseline_3 = baseline_[forecast3 + '_24_' + mode + '_7']\n",
    "    baseline_4 = baseline_[forecast4 + '_24_' + mode + '_7']\n",
    "    baseline_5 = baseline_[forecast5 + '_24_' + mode + '_7']\n",
    "    baseline_6 = baseline_[forecast6 + '_24_' + mode + '_7']\n",
    "    X_test_withBASELINE_total = np.array(test)[index]\n",
    "    \n",
    "    tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "    preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "    train_consensus = np.stack((baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6), axis = 1)\n",
    "    train_consensus_hurr = np.stack((baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6, preds), axis = 1)\n",
    "   \n",
    "if True:\n",
    "    baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "    if basin == 'EP':\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] < 2019].loc[X_test_baseline['YEAR_0'] >= year].loc[\n",
    "            X_test_baseline[forecast6 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast5 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]\n",
    "    index = baseline_.index\n",
    "    baseline_1 = baseline_[forecast + '_24_'+ mode + '_7']\n",
    "    baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    baseline_3 = baseline_[forecast3 + '_24_' + mode + '_7']\n",
    "    baseline_4 = baseline_[forecast4 + '_24_' + mode + '_7']\n",
    "    baseline_5 = baseline_[forecast5 + '_24_' + mode + '_7']\n",
    "    baseline_6 = baseline_[forecast6 + '_24_' + mode + '_7']\n",
    "    X_test_withBASELINE_total = np.array(test)[index]\n",
    "    \n",
    "    tgt_test = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "    preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "    test_consensus = np.stack((baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6), axis = 1)\n",
    "    test_consensus_hurr = np.stack((baseline_1, baseline_2, baseline_3, baseline_4, baseline_5, baseline_6, preds), axis = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-c169a7e7bf3e>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-c169a7e7bf3e>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))print(m.coef_)\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#AN\n",
    "m = Lasso(alpha = 10)\n",
    "m.fit(train_consensus, tgt_)\n",
    "preds = np.array(m.predict(test_consensus))\n",
    "\n",
    "print(\"consensus without Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))print(m.coef_)\n",
    "\n",
    "m = Lasso(alpha = 3)\n",
    "m.fit(train_consensus_hurr, tgt_)\n",
    "preds = np.array(m.predict(test_consensus_hurr))\n",
    "\n",
    "\n",
    "print(\"consensus with Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))print(m.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consensus without Hurricast 10.62 14.51 14.38\n",
      "consensus with Hurricast 10.62 14.51 14.38\n",
      "[0.         0.223805   0.7595393  0.         0.12981753 0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "#AN\n",
    "m = Lasso(alpha = 30)\n",
    "m.fit(train_consensus, tgt_)\n",
    "preds = np.array(m.predict(test_consensus))\n",
    "\n",
    "print(\"consensus without Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "\n",
    "m2 = Lasso(alpha = 30)\n",
    "m2.fit(train_consensus_hurr, tgt_)\n",
    "preds = np.array(m2.predict(test_consensus_hurr))\n",
    "\n",
    "\n",
    "print(\"consensus with Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "print(m2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consensus without Hurricast 10.15 13.71 16.92\n",
      "consensus with Hurricast 10.15 13.71 16.92\n"
     ]
    }
   ],
   "source": [
    "#EP\n",
    "m = Lasso(alpha = 50)\n",
    "m.fit(train_consensus, tgt_)\n",
    "#print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_test), np.array(m.predict(test_consensus))))\n",
    "#print(m.coef_)\n",
    "preds = np.array(m.predict(test_consensus))\n",
    "print(\"consensus without Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "\n",
    "m = Lasso(alpha = 50)\n",
    "m.fit(train_consensus_hurr, tgt_)\n",
    "#print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_test), np.array(m.predict(test_consensus_hurr))))\n",
    "#print(m.coef_)\n",
    "preds = np.array(m.predict(test_consensus_hurr))\n",
    "print(\"consensus with Hurricast\",\n",
    "                np.around(mean_absolute_error(np.array(tgt_test), preds), decimals=2),\n",
    "                np.around(np.std(tgt_test - preds), decimals=2),\n",
    "                np.around(sum(abs(tgt_test - preds) > 20)/len(preds) * 100, decimals = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-808056b18fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_train5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_total = RandomForestRegressor(max_depth=8, n_estimators=250, criterion = 'mae')\n",
    "xgb_total.fit(train, tgt_train5)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  42\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.55 14.03 16.67\n",
      "Hurr until 2012:  11.55 14.03 16.67\n",
      "SHIP 10.74 12.94 11.9\n",
      "HWRF 7.74 10.23 14.29\n",
      "OFCL 8.45 10.79 2.38\n",
      "FSSE 6.64 8.01 0.0\n",
      "GFDL 17.38 18.83 33.33\n",
      "DSHP 10.74 12.94 11.9\n"
     ]
    }
   ],
   "source": [
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFDL', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.41 22.0 36.05\n",
      "Hurr until 2012:  17.41 22.0 36.05\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.72 15.45 16.29\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.21 22.03 31.65\n",
      "Hurr until 2012:  17.21 22.03 31.65\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.09 15.46 19.78\n"
     ]
    }
   ],
   "source": [
    "#TRAIN UNTIL 2012\n",
    "#AN\n",
    "#xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.07, subsample=0.8, min_child_weight=5, colsample_bytree = 0.7)\n",
    "#EP\n",
    "#xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.05, subsample=0.7, min_child_weight=5, colsample_bytree = 0.7)\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.07, subsample=0.8, min_child_weight=5, colsample_bytree = 0.7)\n",
    "xgb_total.fit(X_train_total, tgt_intensity_train)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_total, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.41 21.86 36.74\n",
      "Hurr until 2012:  17.41 21.86 36.74\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.77 15.43 16.12\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  16.58 21.59 31.21\n",
      "Hurr until 2012:  16.58 21.59 31.21\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.06 15.43 19.78\n"
     ]
    }
   ],
   "source": [
    "#TABULAR ONLY\n",
    "#train2 = np.concatenate((X_train, X_test[:m]), axis = 0)\n",
    "#tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:17094]), axis = 0)\n",
    "xgb_total = XGBRegressor(max_depth=7, n_estimators=120, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_total.fit(np.array(X_train), tgt_intensity_train)\n",
    "compare_perf_intensity_per_year_6cast(np.array(X_test), dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(np.array(X_test), dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104651, 128)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  21.03 26.67 40.38\n",
      "Hurr until 2012:  21.03 26.67 40.38\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.91 15.81 16.64\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  20.17 24.91 45.05\n",
      "Hurr until 2012:  20.17 24.91 45.05\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.35 15.7 21.32\n"
     ]
    }
   ],
   "source": [
    "#EMBED ONLY\n",
    "#train2 = np.concatenate((X_train_embed, X_test_embed[:m]), axis = 0)\n",
    "#tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:17094]), axis = 0)\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_total.fit(np.array(X_train_embed), tgt_intensity_train)\n",
    "compare_perf_intensity_per_year_6cast(np.array(X_test_embed), dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(np.array(X_test_embed), dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.91 17.79 19.93\n",
      "Hurr until 2012:  12.91 17.79 19.93\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.34 15.15 14.9\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.94 15.01 18.46\n",
      "Hurr until 2012:  11.94 15.01 18.46\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.49 14.79 16.7\n"
     ]
    }
   ],
   "source": [
    "#USING OFCL\n",
    "train = np.concatenate((X_test_baseline[:m], X_test_embed[:m]), axis = 1)\n",
    "tgt_train3 = tgt_intensity_test[:m]\n",
    "test = np.concatenate((X_test_baseline, X_test_embed), axis = 1)\n",
    "xgb_total = XGBRegressor(max_depth=4, n_estimators=300, learning_rate=0.03, subsample=0.8, min_child_weight=1)\n",
    "xgb_total.fit(train, tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.81 15.84 17.5\n",
      "Hurr until 2012:  11.81 15.84 17.5\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.11 14.86 14.9\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.66 14.4 18.46\n",
      "Hurr until 2012:  11.66 14.4 18.46\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.39 14.62 16.7\n"
     ]
    }
   ],
   "source": [
    "#OFCL ONLY\n",
    "#train = np.array(X_test_baseline[:17094])\n",
    "#tgt_train3 = tgt_intensity_test[:17094]\n",
    "train = X_test_baseline[:m].loc[X_test_baseline['cat_basin_AN_0'] + X_test_baseline['cat_basin_EP_0'] == 1]\n",
    "tgt_train3 = tgt_intensity_test[train.index]\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=300, learning_rate=0.03, subsample=0.7, min_child_weight=1)\n",
    "xgb_total.fit(np.array(train), tgt_train3)\n",
    "#xgb_total = XGBRegressor(max_depth=7, n_estimators=300, learning_rate=0.03, subsample=0.55, min_child_weight=1)\n",
    "#xgb_total.fit(np.array(train), tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(X_test_baseline, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_baseline, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "#compare_perf_intensity_per_year_4cast(X_test_baseline, dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_OFCL':[], 'std_OFCL':[], 'MAES_FSSE':[], 'std_FSSE':[]}, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', mode='vmax', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', year = 2017)\n",
    "#xgb_total = XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.05, subsample=0.7, min_child_weight=7)\n",
    "#xgb_total.fit(train, tgt_train3)\n",
    "#compare_perf_intensity_per_year_4cast(X_test_baseline, dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_OFCL':[], 'std_OFCL':[], 'MAES_FSSE':[], 'std_FSSE':[]}, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', mode='vmax', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.13 16.64 16.29\n",
      "Hurr until 2012:  12.13 16.64 16.29\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.23 15.03 14.9\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.02 15.09 18.68\n",
      "Hurr until 2012:  12.02 15.09 18.68\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.53 14.77 17.8\n"
     ]
    }
   ],
   "source": [
    "#OFCL with AN EP ONLY\n",
    "train_0 = X_test_baseline[:m].loc[X_test_baseline['cat_basin_AN_0'] + X_test_baseline['cat_basin_EP_0'] == 1]\n",
    "train = np.concatenate((train_0, X_test_total[train_0.index]), axis = 1)\n",
    "tgt_train3 = tgt_intensity_test[train_0.index]\n",
    "test = np.concatenate((X_test_baseline, X_test_total), axis = 1)\n",
    "#xgb_total = XGBRegressor(max_depth=5, n_estimators=150, learning_rate=0.05, subsample=0.8, min_child_weight=1, colsample_bytree = 0.7)\n",
    "#xgb_total.fit(train, tgt_train3)\n",
    "xgb_total = XGBRegressor(max_depth=7, n_estimators=300, learning_rate=0.03, subsample=0.55, min_child_weight=1)\n",
    "xgb_total.fit(np.array(train), tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.12 16.46 17.16\n",
      "Hurr until 2012:  12.12 16.46 17.16\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.22 15.0 14.56\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.31 14.52 16.48\n",
      "Hurr until 2012:  11.31 14.52 16.48\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.47 14.72 17.36\n"
     ]
    }
   ],
   "source": [
    "#OFCL with AN EP ONLY\n",
    "train_0 = X_test_baseline[:m].loc[X_test_baseline['cat_basin_AN_0'] + X_test_baseline['cat_basin_EP_0'] == 1]\n",
    "train = np.concatenate((train_0, X_test_total[train_0.index]), axis = 1)\n",
    "tgt_train3 = tgt_intensity_test[train_0.index]\n",
    "test = np.concatenate((X_test_baseline, X_test_total), axis = 1)\n",
    "#xgb_total = XGBRegressor(max_depth=5, n_estimators=150, learning_rate=0.05, subsample=0.8, min_child_weight=1, colsample_bytree = 0.7)\n",
    "#xgb_total.fit(train, tgt_train3)\n",
    "xgb_total = XGBRegressor(max_depth=6, n_estimators=200, learning_rate=0.03, subsample=0.8, min_child_weight=3, colsample_bytree = 1)\n",
    "xgb_total.fit(train, tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "#xgb_total = XGBRegressor(max_depth=7, n_estimators=200, learning_rate=0.03, subsample=0.8, min_child_weight=3, colsample_bytree = 1)\n",
    "#xgb_total.fit(train, tgt_train3)\n",
    "#reached 9.1\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.18 16.7 17.33\n",
      "Hurr until 2012:  12.18 16.7 17.33\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.23 15.02 14.56\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.12 15.25 20.0\n",
      "Hurr until 2012:  12.12 15.25 20.0\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 11.56 14.82 18.24\n"
     ]
    }
   ],
   "source": [
    "#OFCL with AN EP ONLY\n",
    "train_0 = X_test_baseline[:m].loc[X_test_baseline['cat_basin_AN_0'] + X_test_baseline['cat_basin_EP_0'] == 1]\n",
    "train = np.concatenate((train_0, X_test_embed[train_0.index]), axis = 1)\n",
    "tgt_train3 = tgt_intensity_test[train_0.index]\n",
    "test = np.concatenate((X_test_baseline, X_test_embed), axis = 1)\n",
    "xgb_total = XGBRegressor(max_depth=6, n_estimators=300, learning_rate=0.03, subsample=0.55, min_child_weight=1)\n",
    "xgb_total.fit(np.array(train), tgt_train3)\n",
    "#xgb_total = XGBRegressor(max_depth=6, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=2)\n",
    "#xgb_total.fit(train, tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97493, 640)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features kept 196\n",
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.44 22.17 34.49\n",
      "Hurr until 2012:  17.44 22.17 34.49\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.7 15.45 16.12\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  16.62 21.67 31.87\n",
      "Hurr until 2012:  16.62 21.67 31.87\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.06 15.47 20.0\n"
     ]
    }
   ],
   "source": [
    "#problem with NaN\n",
    "#xgb2 = XGBRegressor(max_depth=8, n_estimators=140, learning_rate = 0.07, subsample = 0.7, min_child_weight = 5)\n",
    "#xgb2.fit(train, tgt_train3)\n",
    "m = Lasso(alpha = 0.001)\n",
    "m.fit(train_scaled, tgt_train5)\n",
    "select = SelectFromModel(m, threshold = '0.25*mean', prefit=True)\n",
    "X_train_sparse = select.transform(train_scaled)\n",
    "X_test_sparse = select.transform(test_scaled)\n",
    "print(\"Number of features kept\", X_train_sparse.shape[1])\n",
    "xgb_sparse2 = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.05, subsample=0.7, min_child_weight=1, colsample_bytree = 0.7)\n",
    "xgb_sparse2.fit(X_train_sparse, tgt_train5)\n",
    "compare_perf_intensity_per_year_6cast(X_test_sparse, dict6, xgb_total = xgb_sparse2, xgb_tot = xgb_sparse2, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(X_test_sparse, dict6, xgb_total = xgb_sparse2, xgb_tot = xgb_sparse2, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectFromModel(m, threshold = '0.5mean', prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(train)\n",
    "train_scaled = scaler.transform(train)\n",
    "test_scaled = scaler.transform(X_test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  577\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  17.37 22.23 35.7\n",
      "Hurr until 2012:  17.37 22.23 35.7\n",
      "SHIP 16.15 20.79 31.37\n",
      "HWRF 12.0 16.24 17.16\n",
      "OFCL 11.13 15.3 13.34\n",
      "FSSE 10.73 14.44 14.56\n",
      "GFSO 14.73 18.39 22.7\n",
      "DSHP 14.24 18.38 25.3\n",
      "Consensus ofcl 11.36 15.15 15.42\n",
      "Consensus Hurr 11.69 15.48 15.77\n",
      "Total number of steps for comparison:  455\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  16.9 21.81 32.09\n",
      "Hurr until 2012:  16.9 21.81 32.09\n",
      "SHIP 15.56 19.27 29.67\n",
      "HWRF 11.28 15.48 18.9\n",
      "OFCL 11.7 15.05 15.16\n",
      "FSSE 12.06 15.52 21.1\n",
      "GFSO 17.33 21.75 31.21\n",
      "DSHP 15.44 19.17 29.23\n",
      "Consensus ofcl 11.81 15.07 19.34\n",
      "Consensus Hurr 12.14 15.53 20.22\n"
     ]
    }
   ],
   "source": [
    "#FULL TRAIN WITH CNN/GRU EMBEDS\n",
    "#train = np.concatenate((X_train_total, X_test_total[:m]), axis = 0)\n",
    "#tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[:17094]), axis = 0)\n",
    "\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.03, subsample=0.8, min_child_weight=5)\n",
    "xgb_total.fit(train_scaled, tgt_train5)\n",
    "compare_perf_intensity_per_year_6cast(test_scaled, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(test_scaled, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do slice indexing on <class 'pandas.core.indexes.range.RangeIndex'> with these indexers [Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)] of <class 'sklearn.linear_model.coordinate_descent.Lasso'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-566e33d708e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#FULL TRAIN WITH CNN/GRU EMBEDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_baseline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtgt_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_intensity_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2975\u001b[0m         \u001b[0;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_index_sliceable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mconvert_to_index_sliceable\u001b[0;34m(obj, key)\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   3189\u001b[0m                 return slice(\n\u001b[1;32m   3190\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"slice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3191\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"slice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3192\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"slice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 )\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_validate_indexer\u001b[0;34m(self, form, key, kind)\u001b[0m\n\u001b[1;32m   5069\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5070\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5071\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_invalid_indexer\u001b[0;34m(self, form, key)\u001b[0m\n\u001b[1;32m   3338\u001b[0m             \u001b[0;34m\"cannot do {form} indexing on {klass} with these \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m             \"indexers [{key}] of {kind}\".format(\n\u001b[0;32m-> 3340\u001b[0;31m                 \u001b[0mform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3341\u001b[0m             )\n\u001b[1;32m   3342\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot do slice indexing on <class 'pandas.core.indexes.range.RangeIndex'> with these indexers [Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)] of <class 'sklearn.linear_model.coordinate_descent.Lasso'>"
     ]
    }
   ],
   "source": [
    "#FULL TRAIN WITH CNN/GRU EMBEDS\n",
    "train = np.concatenate((np.array(X_test_baseline[:m]), X_test_embed[:m]), axis = 1)\n",
    "tgt_train3 = tgt_intensity_test[:m]\n",
    "test = np.concatenate((X_test_baseline, X_test_embed), axis = 1)\n",
    "xgb_total = XGBRegressor(max_depth=9, n_estimators=150, learning_rate=0.05, subsample=0.7, min_child_weight=7)\n",
    "xgb_total.fit(train, tgt_train3)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)\n",
    "compare_perf_intensity_per_year_6cast(test, dict6, xgb_total = xgb_total, xgb_tot = xgb_total, basin='EP', forecast='SHIP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP', year = 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  41\n",
      "Year  2012  MAE intensity basin AN Hurricast trained full:  9.13 with std  11.62\n",
      "Year  2012  MAE intensity basin AN Hurricast trained until 2012:  9.19 with std  11.32\n",
      "Year  2012  MAE intensity basin AN Official Forecast SHIP :  9.56 with std  10.79\n",
      "Year  2012  MAE intensity basin AN Official Forecast HWRF :  8.1 with std  11.14\n",
      "Year  2012  MAE intensity basin AN Official Forecast OFCL :  9.02 with std  11.36\n",
      "Year  2012  MAE intensity basin AN Official Forecast FSSE :  10.37 with std  11.62\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  8\n",
      "Year  2013  MAE intensity basin AN Hurricast trained full:  12.06 with std  11.75\n",
      "Year  2013  MAE intensity basin AN Hurricast trained until 2012:  11.28 with std  10.57\n",
      "Year  2013  MAE intensity basin AN Official Forecast SHIP :  19.31 with std  12.82\n",
      "Year  2013  MAE intensity basin AN Official Forecast HWRF :  9.31 with std  9.22\n",
      "Year  2013  MAE intensity basin AN Official Forecast OFCL :  14.06 with std  5.44\n",
      "Year  2013  MAE intensity basin AN Official Forecast FSSE :  8.56 with std  8.28\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  72\n",
      "Year  2014  MAE intensity basin AN Hurricast trained full:  6.73 with std  8.32\n",
      "Year  2014  MAE intensity basin AN Hurricast trained until 2012:  6.36 with std  8.0\n",
      "Year  2014  MAE intensity basin AN Official Forecast SHIP :  7.02 with std  8.36\n",
      "Year  2014  MAE intensity basin AN Official Forecast HWRF :  12.13 with std  13.58\n",
      "Year  2014  MAE intensity basin AN Official Forecast OFCL :  6.7 with std  8.39\n",
      "Year  2014  MAE intensity basin AN Official Forecast FSSE :  8.16 with std  8.09\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  122\n",
      "Year  2015  MAE intensity basin AN Hurricast trained full:  8.29 with std  11.25\n",
      "Year  2015  MAE intensity basin AN Hurricast trained until 2012:  8.48 with std  11.76\n",
      "Year  2015  MAE intensity basin AN Official Forecast SHIP :  9.52 with std  12.95\n",
      "Year  2015  MAE intensity basin AN Official Forecast HWRF :  9.44 with std  13.77\n",
      "Year  2015  MAE intensity basin AN Official Forecast OFCL :  7.93 with std  11.75\n",
      "Year  2015  MAE intensity basin AN Official Forecast FSSE :  8.0 with std  11.84\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  231\n",
      "Year  2016  MAE intensity basin AN Hurricast trained full:  11.42 with std  14.84\n",
      "Year  2016  MAE intensity basin AN Hurricast trained until 2012:  11.36 with std  14.99\n",
      "Year  2016  MAE intensity basin AN Official Forecast SHIP :  10.38 with std  14.57\n",
      "Year  2016  MAE intensity basin AN Official Forecast HWRF :  10.11 with std  14.34\n",
      "Year  2016  MAE intensity basin AN Official Forecast OFCL :  8.42 with std  11.4\n",
      "Year  2016  MAE intensity basin AN Official Forecast FSSE :  8.87 with std  12.22\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  228\n",
      "Year  2017  MAE intensity basin AN Hurricast trained full:  12.37 with std  15.73\n",
      "Year  2017  MAE intensity basin AN Hurricast trained until 2012:  12.42 with std  15.8\n",
      "Year  2017  MAE intensity basin AN Official Forecast SHIP :  12.32 with std  17.27\n",
      "Year  2017  MAE intensity basin AN Official Forecast HWRF :  10.05 with std  12.65\n",
      "Year  2017  MAE intensity basin AN Official Forecast OFCL :  8.59 with std  11.83\n",
      "Year  2017  MAE intensity basin AN Official Forecast FSSE :  7.72 with std  10.07\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  236\n",
      "Year  2018  MAE intensity basin AN Hurricast trained full:  9.78 with std  12.7\n",
      "Year  2018  MAE intensity basin AN Hurricast trained until 2012:  9.89 with std  12.82\n",
      "Year  2018  MAE intensity basin AN Official Forecast SHIP :  10.79 with std  14.97\n",
      "Year  2018  MAE intensity basin AN Official Forecast HWRF :  8.8 with std  12.37\n",
      "Year  2018  MAE intensity basin AN Official Forecast OFCL :  8.53 with std  12.1\n",
      "Year  2018  MAE intensity basin AN Official Forecast FSSE :  8.82 with std  12.16\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  208\n",
      "Year  2019  MAE intensity basin AN Hurricast trained full:  10.36 with std  13.38\n",
      "Year  2019  MAE intensity basin AN Hurricast trained until 2012:  10.52 with std  13.46\n",
      "Year  2019  MAE intensity basin AN Official Forecast SHIP :  10.89 with std  14.22\n",
      "Year  2019  MAE intensity basin AN Official Forecast HWRF :  9.89 with std  12.68\n",
      "Year  2019  MAE intensity basin AN Official Forecast OFCL :  8.55 with std  11.53\n",
      "Year  2019  MAE intensity basin AN Official Forecast FSSE :  8.73 with std  11.42\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAES_2012': [9.19, 11.28, 6.36, 8.48, 11.36, 12.42, 9.89, 10.52],\n",
       " 'MAES_FSSE': [10.37, 8.56, 8.16, 8.0, 8.87, 7.72, 8.82, 8.73],\n",
       " 'MAES_HWRF': [8.1, 9.31, 12.13, 9.44, 10.11, 10.05, 8.8, 9.89],\n",
       " 'MAES_OFCL': [9.02, 14.06, 6.7, 7.93, 8.42, 8.59, 8.53, 8.55],\n",
       " 'MAES_SHIP': [9.56, 19.31, 7.02, 9.52, 10.38, 12.32, 10.79, 10.89],\n",
       " 'MAEs_full': [9.13, 12.06, 6.73, 8.29, 11.42, 12.37, 9.78, 10.36],\n",
       " 'num_samples': [41, 8, 72, 122, 231, 228, 236, 208],\n",
       " 'std_2012': [11.32, 10.57, 8.0, 11.76, 14.99, 15.8, 12.82, 13.46],\n",
       " 'std_FSSE': [11.62, 8.28, 8.09, 11.84, 12.22, 10.07, 12.16, 11.42],\n",
       " 'std_HWRF': [11.14, 9.22, 13.58, 13.77, 14.34, 12.65, 12.37, 12.68],\n",
       " 'std_OFCL': [11.36, 5.44, 8.39, 11.75, 11.4, 11.83, 12.1, 11.53],\n",
       " 'std_SHIP': [10.79, 12.82, 8.36, 12.95, 14.57, 17.27, 14.97, 14.22],\n",
       " 'std_full': [11.62, 11.75, 8.32, 11.25, 14.84, 15.73, 12.7, 13.38],\n",
       " 'year': [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xgb_intensity_all_years_full_train_4cast(forecast = 'SHIP', basin_only = False, sparse = False, max_depth = 8, n_estimators = 100, learning_rate = 0.07, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  0\n",
      "Year 2012 Basin AN MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2012\n",
      "Total number of steps for comparison:  0\n",
      "Year 2013 Basin AN MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2013\n",
      "Total number of steps for comparison:  0\n",
      "Year 2014 Basin AN MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2014\n",
      "Total number of steps for comparison:  113\n",
      "Year 2015 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  8.25 11.46 11.5\n",
      "Hurr until 2012:  8.48 11.83 11.5\n",
      "SHIP 9.88 13.33 11.5\n",
      "HWRF 9.86 14.21 9.73\n",
      "OFCL 8.5 12.2 8.85\n",
      "FSSE 8.35 12.25 10.62\n",
      "GFSO 13.36 16.04 22.12\n",
      "DSHP 9.87 13.33 11.5\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  230\n",
      "Year 2016 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.49 15.0 16.09\n",
      "Hurr until 2012:  11.41 15.03 15.65\n",
      "SHIP 10.36 14.57 11.74\n",
      "HWRF 10.06 14.32 13.48\n",
      "OFCL 8.37 11.36 4.78\n",
      "FSSE 8.83 12.2 7.83\n",
      "GFSO 14.74 18.35 23.48\n",
      "DSHP 10.1 13.78 10.87\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  227\n",
      "Year 2017 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.17 15.41 20.26\n",
      "Hurr until 2012:  12.3 15.56 18.94\n",
      "SHIP 12.13 16.94 19.82\n",
      "HWRF 10.06 12.67 12.33\n",
      "OFCL 8.56 11.82 6.61\n",
      "FSSE 7.74 10.09 5.73\n",
      "GFSO 17.29 17.77 31.28\n",
      "DSHP 10.02 13.38 13.66\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  234\n",
      "Year 2018 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  9.8 12.75 12.82\n",
      "Hurr until 2012:  9.82 12.8 12.82\n",
      "SHIP 10.84 15.02 14.53\n",
      "HWRF 8.85 12.42 9.83\n",
      "OFCL 8.56 12.13 8.12\n",
      "FSSE 8.87 12.21 8.97\n",
      "GFSO 11.21 12.87 14.96\n",
      "DSHP 10.24 13.42 14.1\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  208\n",
      "Year 2019 Basin AN MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  10.35 13.28 10.58\n",
      "Hurr until 2012:  10.42 13.34 12.02\n",
      "SHIP 10.89 14.22 17.79\n",
      "HWRF 9.89 12.68 11.54\n",
      "OFCL 8.55 11.53 7.21\n",
      "FSSE 8.73 11.42 7.21\n",
      "GFSO 13.54 15.47 21.15\n",
      "DSHP 10.6 13.9 16.83\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAES_2012': [8.48, 11.41, 12.3, 9.82, 10.42],\n",
       " 'MAES_DSHP': [9.87, 10.1, 10.02, 10.24, 10.6],\n",
       " 'MAES_FSSE': [8.35, 8.83, 7.74, 8.87, 8.73],\n",
       " 'MAES_GFSO': [13.36, 14.74, 17.29, 11.21, 13.54],\n",
       " 'MAES_HWRF': [9.86, 10.06, 10.06, 8.85, 9.89],\n",
       " 'MAES_OFCL': [8.5, 8.37, 8.56, 8.56, 8.55],\n",
       " 'MAES_SHIP': [9.88, 10.36, 12.13, 10.84, 10.89],\n",
       " 'MAEs_full': [8.25, 11.49, 12.17, 9.8, 10.35],\n",
       " 'num_samples': [113, 230, 227, 234, 208],\n",
       " 'std_2012': [11.83, 15.03, 15.56, 12.8, 13.34],\n",
       " 'std_DSHP': [13.33, 13.78, 13.38, 13.42, 13.9],\n",
       " 'std_FSSE': [12.25, 12.2, 10.09, 12.21, 11.42],\n",
       " 'std_GFSO': [16.04, 18.35, 17.77, 12.87, 15.47],\n",
       " 'std_HWRF': [14.21, 14.32, 12.67, 12.42, 12.68],\n",
       " 'std_OFCL': [12.2, 11.36, 11.82, 12.13, 11.53],\n",
       " 'std_SHIP': [13.33, 14.57, 16.94, 15.02, 14.22],\n",
       " 'std_full': [11.46, 15.0, 15.41, 12.75, 13.28],\n",
       " 'year': [2015, 2016, 2017, 2018, 2019]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xgb_intensity_all_years_full_train_6cast(forecast = 'SHIP', max_depth = 8, n_estimators = 150, learning_rate = 0.07, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  0\n",
      "Year 2012 Basin EP MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2012\n",
      "Total number of steps for comparison:  0\n",
      "Year 2013 Basin EP MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2013\n",
      "Total number of steps for comparison:  0\n",
      "Year 2014 Basin EP MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2014\n",
      "Total number of steps for comparison:  255\n",
      "Year 2015 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  11.65 16.07 17.65\n",
      "Hurr until 2012:  11.83 16.22 17.65\n",
      "SHIP 12.75 17.89 17.65\n",
      "HWRF 11.79 17.29 16.86\n",
      "OFCL 10.42 15.35 11.37\n",
      "FSSE 9.93 13.96 11.76\n",
      "GFSO 15.62 20.14 25.49\n",
      "DSHP 12.41 16.99 17.65\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  315\n",
      "Year 2016 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  9.64 13.24 13.97\n",
      "Hurr until 2012:  9.95 13.44 14.29\n",
      "SHIP 9.87 12.84 10.79\n",
      "HWRF 10.38 13.74 11.75\n",
      "OFCL 8.72 11.89 6.67\n",
      "FSSE 8.51 11.53 8.57\n",
      "GFSO 13.02 16.38 21.27\n",
      "DSHP 9.69 12.58 10.16\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  167\n",
      "Year 2017 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  9.02 12.96 8.98\n",
      "Hurr until 2012:  9.1 13.04 10.18\n",
      "SHIP 9.97 13.0 10.78\n",
      "HWRF 7.87 11.61 8.38\n",
      "OFCL 8.14 11.67 4.79\n",
      "FSSE 7.92 10.67 4.79\n",
      "GFSO 11.93 16.27 17.96\n",
      "DSHP 9.49 12.71 10.18\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  355\n",
      "Year 2018 Basin EP MAE intensity | std | busts > 20kn %\n",
      "Hurr full:  12.09 15.73 20.0\n",
      "Hurr until 2012:  12.4 16.1 21.13\n",
      "SHIP 12.91 15.83 19.72\n",
      "HWRF 10.94 14.86 16.9\n",
      "OFCL 10.22 13.48 9.3\n",
      "FSSE 9.83 12.64 12.39\n",
      "GFSO 19.32 20.45 38.59\n",
      "DSHP 12.96 15.85 20.0\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  0\n",
      "Year 2019 Basin EP MAE intensity | std | busts > 20kn %\n",
      "\n",
      " No forecasts for year  2019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAES_2012': [11.83, 9.95, 9.1, 12.4],\n",
       " 'MAES_DSHP': [12.41, 9.69, 9.49, 12.96],\n",
       " 'MAES_FSSE': [9.93, 8.51, 7.92, 9.83],\n",
       " 'MAES_GFSO': [15.62, 13.02, 11.93, 19.32],\n",
       " 'MAES_HWRF': [11.79, 10.38, 7.87, 10.94],\n",
       " 'MAES_OFCL': [10.42, 8.72, 8.14, 10.22],\n",
       " 'MAES_SHIP': [12.75, 9.87, 9.97, 12.91],\n",
       " 'MAEs_full': [11.65, 9.64, 9.02, 12.09],\n",
       " 'num_samples': [255, 315, 167, 355],\n",
       " 'std_2012': [16.22, 13.44, 13.04, 16.1],\n",
       " 'std_DSHP': [16.99, 12.58, 12.71, 15.85],\n",
       " 'std_FSSE': [13.96, 11.53, 10.67, 12.64],\n",
       " 'std_GFSO': [20.14, 16.38, 16.27, 20.45],\n",
       " 'std_HWRF': [17.29, 13.74, 11.61, 14.86],\n",
       " 'std_OFCL': [15.35, 11.89, 11.67, 13.48],\n",
       " 'std_SHIP': [17.89, 12.84, 13.0, 15.83],\n",
       " 'std_full': [16.07, 13.24, 12.96, 15.73],\n",
       " 'year': [2015, 2016, 2017, 2018]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xgb_intensity_all_years_full_train_6cast(forecast = 'SHIP', max_depth = 8, n_estimators = 150, learning_rate = 0.07, subsample = 0.7, min_child_weight=5, basin = 'EP', forecast2 = 'HWRF', forecast3 = 'OFCL', forecast4 = 'FSSE', forecast5 = 'GFSO', forecast6 = 'DSHP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
